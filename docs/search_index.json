[
["index.html", "Modelos Lineales Modelos Lineales 2021-2 0.1 Detalles 0.2 Complementos Licencia", " Modelos Lineales Sofía Villers Gómez David Alberto Mateos Montes de Oca Modelos Lineales 2021-2 Primera edición del libro de texto para el curso Modelos Lineales de la Facultad de Ciencias. 0.1 Detalles Este libro fue escrito con bookdown usando RStudio. Esta versión fue escrita con: ## Finding R package dependencies ... Done! ## setting value ## version R version 4.0.2 (2020-06-22) ## os Windows 10 x64 ## system x86_64, mingw32 ## ui RStudio ## language (EN) ## collate Spanish_Mexico.1252 ## ctype Spanish_Mexico.1252 ## tz America/Mexico_City ## date 2021-07-19 0.2 Complementos 0.2.1 Ejercicios Interactivos Los ejercicios/ejemplos que acompañan este libro están disponibles a través de un paquete de R cuya instalación se hace de la siguiente manera: # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;alberto-mateos-mo/ModelosLinealesFC&quot;) Los ejercicios contenidos en el paquete están construidos con learnr y las indicaciones para abrirlos se dan al final de cada capítulo del libro. El objetivo de los mismos es proveer al estudiante de los elementos prácticos necesarios para aplicar los diferentes modelos estudiados usando R y al mismo tiempo aprender a interpretar/usar los resultados obtenidos. 0.2.2 Libro sobre R Aunque en los ejercicios interactivos disponibles se abordan las aplicaciones en R, éstos no son exhaustivos en el estudio del lenguaje como tal. Para ello, hemos diseñado un libro dedicado al aprendizaje de R. Este libro puede ser consultado en esta liga. 0.2.3 Aplicación de análisis de datos Hemos desarrollado una aplicación para realizar algunos análisis estadísticos a través de una interfaz gráfica que usaremos como recurso didáctico y que podrán usar para experimentar con algunos modelos sin la necesidad de escribir código. La instalación de la app se hace con el siguiente comando: # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;alberto-mateos-mo/data.analyseR&quot;) Para lanzar la app debe usarse el siguiente comando: data.analyseR::run_app() Licencia This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Share—copy and redistribute the material in any medium or format Remix—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike—If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],
["modelos-lineales.html", "1 Modelos lineales 1.1 Principios de la modelización estadística 1.2 Identificar y Caracterizar Variables 1.3 Tipos de variables y tipo de modelo 1.4 El modelo lineal general", " 1 Modelos lineales Los modelos lineales son una de las herramientas más importantes del análisis cuantitativo. En forma general, estos modelos se utilizan cuando se busca predecir o explicar una variable dependiente a partir de una o más variables independientes. Dependiendo de las características de las variables disponibles, la herramienta estadística más a apropiada para construir el modelo puede variar. Es por esta razón que cuando se construye un modelo estadístico es necesario tener en cuenta los principios de modelización y las caracteristicas de las variables. 1.1 Principios de la modelización estadística Dado un conjunto de variables, cada una de las cuales es un vector de lecturas de un rasgo específico de las muestras en un experimento. Problema: ¿De qué manera una variable \\(Y\\) depende de otras variables \\(X_1,...,X_n\\) en el estudio? Un modelo estadístico define una relación matemática entre los \\(X_i\\) y \\(Y\\). El modelo es una representación del real \\(Y\\) que pretende reemplazarlo en la medida de lo posible. Al menos el modelo debería capturar la dependencia de \\(Y\\) de los \\(X_i\\). 1.2 Identificar y Caracterizar Variables Este es el primer paso en el modelado: Qué variable es la variable de respuesta; Qué variables son las variables explicativas; ¿Son las variables explicativas continuas, categóricas o una mezcla de ambas? ¿Cuál es la naturaleza de la variable de respuesta? ¿Es una medición contínua, un conteo, una proporción, una categoría o un tiempo hasta un evento? 1.3 Tipos de variables y tipo de modelo 1.3.1 En función de las variables explicativas Variables Explicativas Modelo Todas las variables explicativas continuas Regresión Todas las variables explicativas categóricas Análisis de Varianza (ANOVA) Variables explicativas tanto continuas como categóricas Regresión, Análisis de Covarianza (ANCOVA) 1.3.2 En función de la variable respuesta Variable de respuesta Tipo de modelo Continua Regresión normal, ANOVA, ANCOVA Proporción Regresión logística Conteos Modelo log-lineal (regresión Poisson) Binario Regresión logística binaria Tiempo hasta evento Análisis de supervivencia 1.4 El modelo lineal general Como se mencionó, un modelo lineal, es un modelo para el análisis de regresión, que tiene como objetivo determinar una función matemática que describa el comportamiento de una variable dados los valores de otra u otras variables. En particular el análisis de regresión simple, pretende estudiar y explicar el comportamiento de una variable que notamos \\(y\\), y que llamaremos variable respuesta, variable dependiente o variable de interés, a partir de otra variable, que notamos \\(x\\), y que llamamos variable explicativa, variable independiente, covariable o regresor. El principal objetivo de la regresión es encontrar la función que mejor explique la relación entre la variable dependiente y las independientes. Una forma muy general para el modelo sería \\[ y = f(x_1,x_2,...,x_p) + \\epsilon, \\] donde \\(f\\) es una función desconocida y \\(\\epsilon\\) es el error en esta representación. Puesto que normalmente no tenemos suficientes datos para intentar estimar \\(f\\) directamente, normalmente tenemos que asumir que tiene alguna forma restringida. La forma más simple y común es el modelo lineal (LM). \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_z + \\epsilon, \\] donde \\(\\beta_i\\) \\(i=0,1,2\\) son parámetros desconocidos. \\(\\beta_0\\) se llama el término de intercepción. Por lo tanto, el problema se reduce a la estimación de cuatro valores en lugar de los complicados e infinitos \\(f\\) dimensionales. Un modelo lineal simple con una sola variable exploratoria se define como: \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\] donde \\(\\hat y\\) son los valores ajustados para \\(\\hat{beta}_0\\) (intercepto) y \\(\\hat{\\beta}_1\\) (pendiente). Luego por un \\(x_i\\) dado obtenemos un \\(\\hat{y}_i\\) que se aproxima a \\(y_i\\). Todos los modelos mencionados en la sección 1.3 pueden escribirse en términos del modelo lineal general. Es posible apreciar que el tema de modelos lineales es bastante amplio y en este curso sólo nos enfocaremos en los modelos de regresión normal múltiple, análisis de varianza y modelos dinámicos. "],
["regresión-lineal.html", "2 Regresión Lineal 2.1 Un poco de história 2.2 Objetivos del análisis de regresión 2.3 El algorítmo de regresión lineal", " 2 Regresión Lineal 2.1 Un poco de história Los primeros problemas prácticos tipo regresión iniciaron en el siglo XVIII, relacionados con la navegación basada en la Astronomía. Legendre desarrolló el método de mínimo cuadrados en 1805. Gauss afirma que él desarrolló este método algunos años antes y demuestra, en 1809, que mínimos cuadrados proporciona una solución óptima cuando los errores se distribuyen normal. Francis Galton acuña el término regresión al utilizar el modelo para explicar el fenómeno de que los hijos de padres altos, tienden a ser altos en su generación, pero no tan altos como lo fueron sus padres en la propia, por lo que hay un efecto de regresión. El modelo de regresión lineal es, probablemente, el modelo de su tipo más conocido en estadística. El modelo de regresión se usa para explicar o modelar la relación entre una sola variable, \\(y\\), llamada dependiente o respuesta, y una o más variables predictoras, independientes, covariables, o explicativas, \\(x_1, x_2, ..., x_p\\). Si \\(p = 1\\), se trata de un modelo de regresión simple y si \\(p &gt; 1\\), de un modelo de regresión múltiple. En este modelo se asume que la variable de respuesta, \\(y\\), es aleatoria y las variables explicativas son fijas, es decir, no aleatorias. La variable de respuesta debe ser continua, pero los regresores pueden tener cualquier escala de medición. 2.2 Objetivos del análisis de regresión Existen varios objetivos dentro del análisis de regresión, entre otros: Determinar el efecto, o relación, entre las variables explicativas y la respuesta. Predicción de una observación futura. Describir de manera general la estructura de los datos. 2.3 El algorítmo de regresión lineal Sea \\(\\Phi: \\mathcal{X} \\rightarrow \\mathbb{R}^N\\) y consideremos la familia de hipótesis lineales \\[H=\\{x\\mapsto w \\cdot \\Phi(x)+b | w\\in\\mathbb{R}^N, b\\in\\mathbb{R}\\}\\] La regresión lineal consiste en buscar la hipótesis \\(h\\in H\\) con el menor error cuadrático medio, es decir, se debe resolver el problema de optimización: \\[\\min \\frac{1}{m}\\sum_{i=1}^{m}(h(x_i)-y_i)^2\\] "],
["regresión-lineal-simple.html", "3 Regresión lineal simple 3.1 Solución al problema de regresión lineal simple 3.2 Mínimos cuadrados ordinarios 3.3 Pruebas de hipótesis 3.4 Interpretación de los parámetros 3.5 Ejercicios", " 3 Regresión lineal simple Para este modelo supondremos que nuestra respuesta, \\(y\\), es explicada únicamente por una covariable, \\(x\\). Entonces, escribimos nuestro modelo como: \\[y^{(i)}=\\beta_0+\\beta_1x^{(i)}+\\epsilon^{(i)},\\ \\ i=1,2,\\dots,n\\] Como podemos observar, se ha propuesto una relación lineal entre la variable \\(y\\) y la variable explicativa \\(x\\), que es nuestro primer supuesto sobre el modelo: La relación funcional entre \\(x\\) y \\(y\\) es una línea recta. Observamos que la relación no es perfecta, ya que se agrega el término de error, \\(\\epsilon\\). Dado que la parte aleatoria del modelo es la variable \\(y\\), asumimos que al error se le “cargan” los errores de medición de \\(y\\), así como las perturbaciones que le pudieran ocasionar los términos omitidos en el modelo. Gauss desarrolló este modelo a partir de la teoría de errores de medición, que es de donde se desprenden los supuestos sobre este término: \\(\\mathbb{E}(\\epsilon^{(i)})=0\\) \\(\\mathbb{V}ar(\\epsilon^{(i)})=\\sigma^2\\) \\(\\mathbb{C}ov(\\epsilon^{(i)},\\epsilon^{(j)})=0, \\ \\forall i\\neq j\\) N.B. Los errores \\(\\epsilon^{(i)}\\) son variables aleatorias no observables. 3.1 Solución al problema de regresión lineal simple 3.2 Mínimos cuadrados ordinarios En una situación real, tenemos \\(n\\) observaciones de la variable de respuesta así como de la variable explicativa, que conforman las parejas de entrenamiento \\((x_i, y_i), \\ i = 1, 2, ..., n\\). Entonces, nuestro objetivo será encontrar la recta que mejor ajuste a los datos observados. Utilizaremos el método de mínimos cuadrados para estimar los parámetros del modelo, que consiste en minimizar la suma de los errores al cuadrado, esto es: \\[\\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n(y_i-(\\beta_0+\\beta_1x^{(i)}))^2\\] Al minimizar la expresión anteriore obtenemos las siguientes expresiones para los estimadores: \\[\\hat{\\beta_1}=\\frac{\\sum_{i=1}^ny_i(x_i-\\bar{x})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\] \\[\\hat{\\beta_0}=\\bar{y}-\\hat{\\beta_1}\\bar{x}\\] Una desventaja del método de mínimos cuadrados, es que no se pueden hacer procesos de inferencia sobre los parámetros de interés \\(\\beta_0\\) y \\(\\beta_1\\); procesos como intervalos de confianza o pruebas de hipótesis. Para subsanar esta deficiencia, es necesario asumir una distribución para el error, \\(\\epsilon_i\\), que, siguiendo la teoría general de errores, se asume que tiene distribución normal, con media cero y varianza \\(\\sigma^2\\). Este supuesto garantiza que las distribuciones de \\(y_i,\\ \\hat{\\beta_0},\\ \\hat{\\beta_1}\\) sean normales, lo que permite tanto la construcción de intervalos de confianza como de pruebas de hipótesis. N.B El estimador de \\(\\sigma^2\\) está dado por \\(\\hat{\\sigma^2}=\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{n-2}\\) 3.3 Pruebas de hipótesis En el modelo de regresión lineal simple, la prueba de hipótesis más importante es determinar si estadísticamente existe la dependencia líneal entre \\(x\\) y \\(y\\), y que no sea producto del muestreo (debido al azar). Es decir, realizar la prueba de hipótesis: \\[H_0:\\beta_1=0 \\ vs.\\ H_a:\\beta_1\\neq 0\\] No rechazar la hipótesis nula, implicaría que la variable \\(x\\) no ayuda a explicar a \\(y\\) o bien que, tal vez, la relación entre estas variables no es lineal. En este modelo, esta última explicación es un poco cuestionable, ya que se parte, de inicio, del diagrama de dispersión de los datos. Si rechazamos la hipótesis nula, implicará que \\(x\\) es importante para explicar la respuesta \\(y\\) y que la relación lineal entre ellas puede ser adecuada. Rechazar esta hipótesis nula, también podría implicar que existe una relación lineal entre las variables pero, tal vez, se pueda mejorar el ajuste con algún otro término no lineal. 3.4 Interpretación de los parámetros Cuando se tiene una recta en el sentido determinista, los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) tienen una interpretación muy clara; \\(\\beta_0\\) se interpreta como el valor de \\(y\\) cuando \\(x\\) es igual a cero y \\(\\beta_1\\) como el cambio que experimenta la variable de respuesta \\(y\\) por unidad de cambio en \\(x\\). La interpretación, desde el punto de vista estadístico, de los parámetros estimados en el modelo de regresión es muy similar: \\(\\hat{\\beta_0}\\) es el promedio esperado de la respuesta \\(y\\) cuando \\(x = 0\\) (este parámetro tendrá una interpretación dentro del modelo, si tiene sentido que \\(x\\) tome el valor cero, de lo contrario, no tiene una interpretación razonable) y \\(\\hat{\\beta_1}\\) es el cambio promedio o cambio esperado en \\(y\\) por unidad de cambio en \\(x\\). 3.5 Ejercicios Para usar los ejercicios del capítulo de regresión lineal simple corra la siguiente línea de código en su sesión de R: learnr::run_tutorial(&quot;regresion_lineal_simple&quot;, package = &quot;ModelosLinealesFC&quot;) "],
["regresión-lineal-múltiple.html", "4 Regresión lineal múltiple 4.1 Solución al problema de regresión lineal múltiple: Ecuaciones normales 4.2 Prueba de hipótesis 4.3 Interpretación de parámetros 4.4 Predicción de nuevos valores 4.5 Coeficiente de determinación 4.6 Evaluación de supuestos 4.7 Aplicación en R", " 4 Regresión lineal múltiple La mayoría de los fenómenos reales son multicausales, por esta razón, un modelo de regresión más acorde a estudios reales es el modelo de regresión lineal múltiple, que es la generalización del modelo simple. En este modelo supondremos que la variable de respuesta, \\(y\\), puede explicarse a través de una colección de \\(k\\) covariables \\(x_1,\\dots,x_k\\). El modelo se escribe de la siguiente manera: \\[y_i = \\beta_0+\\beta_1 x_1^{(i)}+\\beta_2 x_2^{(i)}+\\dots++\\beta_k x_k^{(i)}+\\epsilon_i\\] Al igual que en el caso simple, los parámetros del modelo se pueden estimar por mínimos cuadrados, con el inconveniente de que no se pueden realizar inferencias sobre ellos. Nuevamente, para poder hacer intervalos de confianza y pruebas de hipótesis sobre los verdaderos parámetros hay que suponer que el vector de errores se distribuye normal, en este caso multivariada, es decir: \\[\\epsilon\\sim N_n(0,\\sigma^2\\mathbb{I})\\] Esta estructura del error permite tener las mismas propiedades distribucionales que en regresión simple, es decir, \\(y_i\\) se distribuye normal y \\(\\beta_i\\) tiene distribución normal, facilitando las inferencias sobre cada parámetro y la construcción de intervalos de predicción para las \\(y\\)’s. 4.1 Solución al problema de regresión lineal múltiple: Ecuaciones normales Las expresiones para estimar los parámetros involucrados en el modelo son: \\[\\hat{\\beta}=(X^TX)^{-1}X^Ty\\] \\[\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{n-p}\\] donde \\(p=k+1\\) es el número total de parámetros en el modelo. Tanto en el modelo simple como en el múltiple, la variación total de las \\(y\\)’s se puede descomponer en una parte que explica el modelo, i.e., los \\(k\\) regresores o variables explicativas y otra no explicada por estas variables, llamada error. \\[\\sum_{i=1}^n(y_i-\\bar{y})^2=\\sum_{i=1}^n(\\hat{y_i}-\\bar{y})^2+\\sum_{i=1}^n(\\hat{y_i}-y_i)^2\\] 4.2 Prueba de hipótesis La descomposición anterior ayuda para realizar la importante prueba de hipótesis: \\[H_0:\\beta_1=\\beta_2=\\dots=\\beta_k=0\\ vs.\\ H_a:\\beta_i\\neq0 \\ p.a. \\ i\\] misma que se realiza a través del cociente entre los errores cuadráticos medios: \\[F_0=\\frac{SS_R/k}{SS_E/(n-k-1)}=\\frac{MS_R}{MS_E}\\sim F_{(k,n-k-1)}\\] Esta estadística se desprende de la tabla de análisis de varianza, que es muy similar a la tabla ANOVA que se utiliza para hacer pruebas de hipótesis. En este caso la tabla es: Fuente de variación Grados de libertad Suma de cuadrados Cuadrados medios F Regresión k \\(SS_R\\) \\(MS_R=SS_R/k\\) Error n-k-1 \\(SS_E\\) \\(MS_E=SS_E/(n-k-1)\\) \\(F=\\frac{MS_R}{MS_E}\\) Total n-1 \\(S_{yy}\\) Por lo general, esta estadística rechaza la hipótesis nula, ya que de lo contrario, implicaría que ninguna de las variables contribuye a explicar la respuesta, \\(y\\). Como se puede observar en la hipótesis alternativa, el rechazar \\(H_0\\) solo implica que al menos uno de los regresores contribuye significativamente a explicar \\(y\\). Asimismo, el rechazar \\(H_0\\) no implica que todos contribuyan ni tampoco dice cuál o cuáles contribuyen, por esta razón, una salida estándar de regresión múltiple tiene pruebas individuales sobre la significancia de cada regresor en el modelo. El estadístico para hacer tanto los contrastes de hipótesis como los intervalos de confianza individuales, es: \\[t=\\frac{\\hat{\\beta_i}-\\beta_0^{(i)}}{\\sqrt{\\hat{\\mathbb{V}ar}(\\hat{\\beta_i})}}\\sim t_{(n-p)}\\] Podemos apreciar que los constrastes de hipótesis se pueden hacer contra cualquier valor particular del parámetro \\(\\beta_0^{(i)}\\), en general. No obstante, en las pruebas estándar sobre los parámetros de un modelo, este valor particular es 0, ya que se intenta determinar si la variable asociada al \\(i\\)-ésimo parámetro es estadísticamente significativa para explicar la respuesta. Por lo que el estadístico para este caso es: \\[t=\\frac{\\hat{\\beta_i}}{\\sqrt{\\hat{\\mathbb{V}ar}(\\hat{\\beta_i})}}\\sim t_{(n-p)}\\] De este estadístico se desprenden también los intervalos de confianza para cada parámetro: \\[\\beta_i\\in(\\hat{\\beta_i}\\pm t_{(n-p,1-\\alpha/2)} \\sqrt{\\hat{\\mathbb{V}ar} (\\hat{\\beta_i})})\\] 4.3 Interpretación de parámetros La interpretación de cada parámetro es similar a la del coeficiente de regresión \\(\\hat{\\beta_1}\\) en el modelo simple, anexando la frase: “manteniendo constantes el resto de las variables.” Esto es, \\(\\hat{\\beta_i}\\) es el cambio promedio o cambio esperado en \\(y\\) por unidad de cambio en \\(x_i\\), sin considerar cambio alguno en ninguna de las otras variables dentro del modelo, es decir, suponiendo que estas otras variables permanecen fijas. Esta interpretación es similar a la que se hace de la derivada parcial en un modelo determinista. Nuevamente, la interpretación de \\(\\hat{\\beta_0}\\) estará sujeta a la posibilidad de que, en este caso, todas las variables puedan tomar el valor cero. 4.4 Predicción de nuevos valores Uno de los usos más frecuentes del modelo de regresión es el de predecir un valor de la respuesta para un valor particular de las covariables en el modelo. Si la predicción se realiza para un valor de las covariables dentro del rango de observación de las mismas, se tratará de una interpolación, y si se realiza para un valor fuera de este rango, hablaremos de una extrapolación. En cualquiera de los dos casos, estaremos interesados en dos tipos de predicciones: Predicción de la respuesta media: \\(y_0=\\mathbb{E}(y|X_0)\\) Predicción de una nueva observación: \\(y_0\\) En ambos casos, la estimación puntual es la misma: \\(\\hat{y_0}=X_0^T\\hat{\\beta}\\) Lo que difiere es el intervalo de predicción. Para la respuesta media es: \\(y_0=(\\hat{y_0}\\pm t_{(n-p,1-\\alpha/2)}\\sqrt{\\hat{\\sigma^2}X_0^T(X^TX)^{-1}X_0})\\) Y para predecir una observación: \\(y_0=(\\hat{y_0}\\pm t_{(n-p,1-\\alpha/2)}\\sqrt{\\hat{\\sigma^2}(1+X_0^T(X^TX)^{-1}X_0)})\\) 4.5 Coeficiente de determinación Un primer elemento de juicio sobre el modelo de regresión lo constituye el coeficiente de determinación \\(R^2\\), que es la proporción de variabilidad de las \\(y\\)’s que es explicada por las \\(x\\)’s y que se escribe como: \\[R^2=\\frac{SS_R}{S_{yy}}=1-\\frac{SS_E}{S_{yy}}\\] Una \\(R^2\\) cercana a uno implicaría que mucha de la variabilidad de la respuesta es explicada por el conjunto de regresores incluidos en el modelo. Es deseable tener una \\(R^2\\) grande en nuestro modelo, pero esto no significa, como mucha gente piensa, que ya el modelo está bien ajustado. 4.6 Evaluación de supuestos Los dos modelos de regresión presentados, el simple y el múltiple, se construyeron sobre los supuestos de: La relación funcional entre la variable de respuesta \\(y\\) y cada regresor \\(x_i\\) es lineal La esperanza de los errores es cero, \\(\\mathbb{E}(\\epsilon_i=0)\\) La varianza de los errores es constante, \\(\\mathbb{V}ar(\\epsilon_i) = \\sigma^2\\) Los errores no están correlacionados, \\(\\mathbb{C}ov(\\epsilon_i, \\epsilon_j) = 0;\\ i\\neq j\\) Los errores tienen distribución normal con media cero y varianza \\(\\sigma^2\\) Entonces, para garantizar que el modelo es adecuado, es indispensable verificar estos supuestos. 4.6.1 Residuos Los elementos más importantes para verificar estos supuestos son los residuos, definidos como: \\[e_i=y_i-\\hat{y}_i\\] Estos residuos representan la discrepancia entre la respuesta predicha por el modelo ajustado, \\(\\hat{y}_i\\) y el correspondiente valor observado, \\(y_i\\). En la literatura de regresi ́on lineal existen cuatro tipos de residuos, a saber Residuo crudo: \\(e_i\\) Residuo estandarizado: \\(d_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma}^2}}\\) Residuo estudentizado interno: \\(r_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma}^2(1-h_{ii}})}\\) Residuo estudentizado externo: \\(t_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma_{(-i)}}^2(1-h_{ii})}}\\) Estos residuos se utilizan en los distintos procedimientos para evaluar los supuestos y lo adecuado del ajuste del modelo. La mayoría de las pruebas conocidas para la verificación de los supuestos, son pruebas gráficas. Indudablemente, la prueba más importante es sobre la normalidad de los errores, ya que sobre este supuesto descansan todas la inferencias de este modelo. La manera de verificarlo es a través de la gráfica conocida como QQ-plot o QQ-norm, que grafica los cuantiles teóricos de una distribución normal (eje x) vs. los cuantiles asociados a los residuos. Entonces, si los residuos realmente provienen de una normal, la gráfica debe mostrar la función identidad. Fuertes desviaciones de esta línea darían evidencia de que los errores no se distribuyen normal. 4.6.2 Linealidad de los predictores La manera estándar de evaluar la linealidad de las variables explicativas es a través de la gráfica de cada una de ellas contra los residuos. Si la variable en cuestión ingresa al modelo de manera lineal, esta gráfica debe mostrar un patrón totalmente aleatorio entre los puntos dispuestos en ella. Cuando la variable explicativa es politómica, este tipo de gráficas son poco ilustrativas en este sentido. 4.6.3 Supuestos sobre los errores Si la gráfica entre los valores ajustados y los residuos estandarizados, muestra un patrón aleatorio, es simétrica alrededor del cero y los puntos están comprendidos entre los valores -2 y 2, entonces se tendrá evidencia de que los errores tienen media cero, varianza constante y no están correlacionados. Los métodos mostrados hasta ahora, permiten evaluar el modelo de manera global y no por cada observación dentro del mismo. Dado que una observación puede resultar determinante sobre alguna(s) característica(s) del modelo, es conveniente verificar el impacto que cada observación pueda tener en los distintos aspectos del modelo. Las estadísticas para evaluar el impacto que tiene una observación sobre todo el vector de parámetros, alguno de los regresores y sobre los valores predichos, se basan en la misma idea, que consiste en cuantificar el cambio en la característica de interés con y sin la observación que se está evaluando. 4.6.4 Puntos palanca Antes de presentar las estadísticas que servirán para hacer este diagnóstico, introduciremos un elemento que es común a ellas: la llamada palanca (leverage) de una observación. Recordemos que el ajuste del modelo se expresaba como: \\[\\hat{\\beta}=(X^TX)^{-1}X^Ty \\Rightarrow \\hat{y}=X\\hat{\\beta}=Hy\\] Con \\(H\\) conocida como la matriz sombrero. Un resultado fundamental sobre esta matriz sombrero es: \\[\\mathbb{V}ar(e)=(I-H)\\sigma^2 \\Rightarrow \\mathbb{V}ar(e_i)=(1-h_i)\\sigma^2\\] Con \\(h_i\\) el i-ésimo elemento de la diagonal de la matriz \\(H\\). Observemos que esta palanca sólo depende de \\(X\\), entonces, una observación con una palanca, \\(h_i\\), grande, es aquella con valores extremos en alguna(s) de su(s) covariable(s). Ya que el promedio de las \\(h_i&#39;s\\) es \\(p/n\\), consideraremos una observación con palanca grande si su palanca es mayor a \\(2p/n\\). En este sentido, \\(h_i\\) corresponde a la distancia de Mahalanobis de \\(X\\) definida como \\((X-\\bar{X})^T\\hat{\\Sigma}^{-1}(X-\\bar{X})\\). La dependencia de las estadísticas para el diagnóstico de las observaciones, estriba en que sus cálculos dependen de los valores de la palanca de cada individuo. Estas estadísticas son: Distancia de Cook Dfbetas Dffits Distancia de Cook: Sirve para determinar si una observación es influyente en todo el vector de parámetros. Una observación se considera influyente, si su distancia de Cook sobrepasa el valor uno. Dfbetas: Sirven para determinar si una observación es influyente en alguno de los coeficientes de regresión. Hay un dfbeta por cada parámetro dentro del modelo, incluido, por supuesto, el de la ordenada al origen. La regla de dedo es que la observación \\(i\\) es influyente en el j-ésimo coeficiente de regresión si: \\[|Dfbetas_{j,i}|&gt;\\frac{2}{\\sqrt{n}}\\] Dffits: Se utilizan para determinar si una observación es influyente en la predicción de \\(y\\). Se dice que la i-ésima observación es influyente para predecir \\(y\\), si: \\[|Dffits_i|&gt;2\\sqrt{\\frac{p}{n}}\\] 4.6.5 Multicolinealidad El modelo de regresión lineal múltiple, se construye bajo el supuesto de que los regresores son ortogonales, i.e., son independientes. Desafortunadamente, en la mayoría de las aplicaciones el conjunto de regresores no es ortogonal. Algunas veces, esta falta de ortogonalidad no es seria; sin embargo, en algunas otras los regresores están muy cerca de una perfecta relación lineal, en tales casos las inferencias realizadas a través del modelo de regresión lineal pueden ser erróneas. Cuando hay una cercana dependencia lineal entre los regresores, se dice que estamos en presencia de un problema de multicolinealidad. Efectos de la multicolinealidad: Varianzas de los coeficientes estimados son muy grandes. Los estimadores calculados de distintas sub muestras de la misma población, pueden ser muy diferentes. La significancia de algún regresor se puede ver afectada (volverse no significativo) por que su varianza es más grande de lo que debería ser en realidad o por la correlación de la variable con el resto dentro del modelo. Es común que algún signo de un parámetro cambie, haciendo ilógica su interpretación dentro del modelo. 4.6.5.1 ¿Cómo detectar multicolinealidad? Matriz de correlación. Examinar las correlaciones entre pares de variables: \\[r_{ij}\\ \\ \\ i, j = 1, 2, \\dots, k\\ \\ i\\neq j\\] Pero, si dos o más regresores están linealmente relacionados, es posible que ninguna de las correlaciones entre cada par de variables, sea grande. Factor de inflación de la varianza. \\[VIF_j=(1-R_j^2)^{-1}\\] Con \\(R_j^2\\) el coeficiente de determinación del modelo de regresión entre el j-ésimo regresor, \\(x_j\\) (tomado como variable de respuesta) y el resto de los regresores \\(x_i\\), \\(i\\neq j\\). Experiencias prácticas indican que si algunos de los VIF’s excede a 10, su coeficiente asociado es pobremente estimado por el modelo debido a multicolinealidad. Análisis del eigensistema. Basado en los eigenvalores de la matriz \\(X^TX\\). Número de condición. \\[K=\\frac{\\lambda_{max}}{\\lambda_{min}}\\] Si el número de condición es menor que 100, no existen problemas serios de multicolinealidad. Si está entre 100 y 1000 existe de moderada a fuerte multicolinealidad y si excede a 1000, hay severa multicolinealidad. Índice de condición. \\[k_j=\\frac{\\lambda_{max}}{\\lambda_j}\\] Si el índice de condición es menor que 10, no hay ningún problema. Si está entre 10 y 30, hay moderada multicolinealidad, y si es mayor que 30, existe una fuerte colinealidad en la j-ésima variable en el modelo. N.B. En algunos paquetes estos índices se presentan aplicando la raíz cuadrada a su expresión, entonces hay que extraer raíz a los puntos de corte de los criterios correspondientes. 4.6.6 Relación funcional Un supuesto importante en el modelo de regresión es el que considera que debe existir una relación funcional lineal entre cada regresor y la variable de respuestas. Pero, ¿qué debemos hacer si no se cumple esta relación lineal de la respuesta con alguno(s) de los regresor(es)? Primero, ya dijimos que este supuesto se evalúa realizando la gráfica de dispersión entre los residuos del modelo y los valores de la variable en cuestión. Cuando no hay una asociación lineal entre la respuesta y la covariable, generalmente este diagrama de dispersión muestra un patrón (tendencia) que sugiere qué tipo de transformación se debería hacer a la covariable para lograr linealidad con la respuesta. Debe quedar claro que la transformación puede realizarse a la variable explicativa o a la variable de respuesta. A muchos investigadores no les gusta transformar la respuesta porque argumentan que pierden interpretabilidad del modelo. Aunque esto puede ser cierto, existen transformaciones de la respuesta que pueden regresarse para interpretar el modelo con la respuesta original. Un problema asociado a esta identificación por parte del usuario, es que debe tener experiencia para asociar estas formas a una función analítica específica; hecho no necesariamente cierto. Por lo tanto, requiere de alguna herramienta técnica que pudiera auxiliarlo en esta labor. Un buen auxiliar, en el caso de que se crea que es necesario transformar la respuesta, es usar la llamada trasformación Box-Cox. 4.6.6.1 Transformación Box-Cox La transformación Box-Cox de la respuesta, es una función que sirve para normalizar la distribución del error, estabilizar la varianza de este error y mejorar la relación lineal entre \\(y\\) y las \\(X’s\\). Se define como: \\[y_i^{\\lambda} = \\left\\{ \\begin{array}{ll} \\frac{y_i^{\\lambda-1}}{\\lambda}, &amp; \\lambda \\neq 0;\\\\ ln(y_i), &amp; \\lambda=0 .\\end{array} \\right.\\] La siguiente tabla muestra el rango de valores de \\(\\lambda\\) que estarían asociados a una transformación analítica común. Rango \\(\\lambda\\) Transformación Asociada (-2.5, -1.5] \\(\\frac{1}{y^2}\\) (-1.5, -0.75] \\(\\frac{1}{y}\\) (-0.75, -0.25] \\(\\frac{1}{\\sqrt{y}}\\) (-0.25, 0.25] \\(ln(y)\\) (0.25, 0.75] \\(\\sqrt{y}\\) (0.75, 1.25] \\(y\\) (1.25, 2.5) \\(y^2\\) 4.6.6.2 Transformación Box-Tidwell Box y Tidwell implementan un proceso iterativo para encontrar la mejor transformación de las variables predictoras en el modelo de regresión lineal. Definiendo como \\(X_j^{\\gamma_j}\\) la correspondiente transformación Box-Tidwell de la variable \\(j\\). La tabla anterior para las transfomaciones analíticas de la respuesta, también aplican para estas transformaciones de los predictores. 4.7 Aplicación en R 4.7.1 Ejemplo: Predicción de esperanza de vida Un estudio quiere generar un modelo que permita predecir la esperanza de vida media de los habitantes de una ciudad en función de diferentes variables. Se dispone de información sobre: habitantes, analfabetismo, ingresos, esperanza de vida, asesinatos, nivel de estudios, presencia de heladas, área y densidad poblacional. Usaremos los datos state.x77 disponibles dentro de R que contiene estadísticas para los 50 estados de Estados Unidos. Primero modificamos los nombres de las variables para que sean más amigables y creamos una nueva variable calculada que represente la densidad poblacional del estado. 4.7.1.1 Analizar la relación entre variables El primer paso a la hora de establecer un modelo lineal múltiple es estudiar la relación que existe entre variables. Esta información es crítica a la hora de identificar cuáles pueden ser los mejores predictores para el modelo, qué variables presentan relaciones de tipo no lineal (por lo que no pueden ser incluidas) y para identificar colinealidad entre predictores. A modo complementario, es recomendable representar la distribución de cada variable mediante histogramas. Las dos formas principales de hacerlo son mediante representaciones gráficas (gráficos de dispersión) y el cálculo del coeficiente de correlación de cada par de variables. round(cor(x = datos), 2) ## habitantes ingresos analfabetismo esp_vida asesinatos estudios heladas area densidad_pobl ## habitantes 1.00 0.21 0.11 -0.07 0.34 -0.10 -0.33 0.02 0.25 ## ingresos 0.21 1.00 -0.44 0.34 -0.23 0.62 0.23 0.36 0.33 ## analfabetismo 0.11 -0.44 1.00 -0.59 0.70 -0.66 -0.67 0.08 0.01 ## esp_vida -0.07 0.34 -0.59 1.00 -0.78 0.58 0.26 -0.11 0.09 ## asesinatos 0.34 -0.23 0.70 -0.78 1.00 -0.49 -0.54 0.23 -0.19 ## estudios -0.10 0.62 -0.66 0.58 -0.49 1.00 0.37 0.33 -0.09 ## heladas -0.33 0.23 -0.67 0.26 -0.54 0.37 1.00 0.06 0.00 ## area 0.02 0.36 0.08 -0.11 0.23 0.33 0.06 1.00 -0.34 ## densidad_pobl 0.25 0.33 0.01 0.09 -0.19 -0.09 0.00 -0.34 1.00 multi.hist(x = datos, dcol = c(&quot;blue&quot;, &quot;red&quot;), dlty = c(&quot;dotted&quot;, &quot;solid&quot;), main = &quot;&quot;) Otros paquetes permiten representar a la vez los diagramas de dispersión, los valores de correlación para cada par de variables y la distribución de cada una de las variables, ese es el caso de la función ggpairs. library(GGally) ggpairs(datos, lower = list(continuous = &quot;smooth&quot;), diag = list(continuous = &quot;barDiag&quot;), axisLabels = &quot;none&quot;) Del análisis preliminar se pueden extraer las siguientes conclusiones: Las variables que tienen una mayor relación lineal con la esperanza de vida son: asesinatos (\\(r=-0.78\\)), analfabetismo (\\(r=-0.59\\)) y estudios (\\(r=0.58\\)). Asesinatos y analfabetismo están medianamente correlacionadas (\\(r = 0.7\\)) por lo que posiblemente no sea útil introducir ambos predictores en el modelo. Las variables habitantes, área y densidad poblacional muestran una distribución exponencial, una transformación logarítmica posiblemente haría más normal su distribución. 4.7.1.2 Generar el modelo Sabemos que hay diferentes formas (algoritmos) para llegar al modelo final más adecuado. En este caso se va a emplear el método mixto iniciando el modelo con todas las variables como predictores y realizando la selección de los mejores predictores con la medición Akaike(AIC). modelo &lt;- lm(esp_vida ~ habitantes + ingresos + analfabetismo + asesinatos + estudios + heladas + area + densidad_pobl, data = datos ) summary(modelo) ## ## Call: ## lm(formula = esp_vida ~ habitantes + ingresos + analfabetismo + ## asesinatos + estudios + heladas + area + densidad_pobl, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.47514 -0.45887 -0.06352 0.59362 1.21823 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.995e+01 1.843e+00 37.956 &lt; 2e-16 *** ## habitantes 6.480e-05 3.001e-05 2.159 0.0367 * ## ingresos 2.701e-04 3.087e-04 0.875 0.3867 ## analfabetismo 3.029e-01 4.024e-01 0.753 0.4559 ## asesinatos -3.286e-01 4.941e-02 -6.652 5.12e-08 *** ## estudios 4.291e-02 2.332e-02 1.840 0.0730 . ## heladas -4.580e-03 3.189e-03 -1.436 0.1585 ## area -1.558e-06 1.914e-06 -0.814 0.4205 ## densidad_pobl -1.105e-03 7.312e-04 -1.511 0.1385 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7337 on 41 degrees of freedom ## Multiple R-squared: 0.7501,\tAdjusted R-squared: 0.7013 ## F-statistic: 15.38 on 8 and 41 DF, p-value: 3.787e-10 El modelo con todas las variables introducidas como predictores tiene una \\(R^2\\) alta (\\(0.7501\\)), i.e. es capaz de explicar el \\(75.01%\\) de la variabilidad observada en la esperanza de vida. El p-value del modelo es significativo (\\(3.787e-10\\)) por lo que se puede aceptar que el modelo no es por azar, al menos uno de los coeficientes parciales de regresión es distinto de 0. Muchos de ellos no son significativos, lo que es un indicativo de que podrían no contribuir al modelo. 4.7.1.3 Selección de los mejores predictores En este caso se van a emplear la estrategia de stepwise mixto. El valor matemático empleado para determinar la calidad del modelo va a ser Akaike(AIC). step(object = modelo, direction = &quot;both&quot;, trace = 1) ## Start: AIC=-22.89 ## esp_vida ~ habitantes + ingresos + analfabetismo + asesinatos + ## estudios + heladas + area + densidad_pobl ## ## Df Sum of Sq RSS AIC ## - analfabetismo 1 0.3050 22.373 -24.208 ## - area 1 0.3564 22.425 -24.093 ## - ingresos 1 0.4120 22.480 -23.969 ## &lt;none&gt; 22.068 -22.894 ## - heladas 1 1.1102 23.178 -22.440 ## - densidad_pobl 1 1.2288 23.297 -22.185 ## - estudios 1 1.8225 23.891 -20.926 ## - habitantes 1 2.5095 24.578 -19.509 ## - asesinatos 1 23.8173 45.886 11.707 ## ## Step: AIC=-24.21 ## esp_vida ~ habitantes + ingresos + asesinatos + estudios + heladas + ## area + densidad_pobl ## ## Df Sum of Sq RSS AIC ## - area 1 0.1427 22.516 -25.890 ## - ingresos 1 0.2316 22.605 -25.693 ## &lt;none&gt; 22.373 -24.208 ## - densidad_pobl 1 0.9286 23.302 -24.174 ## - estudios 1 1.5218 23.895 -22.918 ## + analfabetismo 1 0.3050 22.068 -22.894 ## - habitantes 1 2.2047 24.578 -21.509 ## - heladas 1 3.1324 25.506 -19.656 ## - asesinatos 1 26.7071 49.080 13.072 ## ## Step: AIC=-25.89 ## esp_vida ~ habitantes + ingresos + asesinatos + estudios + heladas + ## densidad_pobl ## ## Df Sum of Sq RSS AIC ## - ingresos 1 0.132 22.648 -27.598 ## - densidad_pobl 1 0.786 23.302 -26.174 ## &lt;none&gt; 22.516 -25.890 ## - estudios 1 1.424 23.940 -24.824 ## + area 1 0.143 22.373 -24.208 ## + analfabetismo 1 0.091 22.425 -24.093 ## - habitantes 1 2.332 24.848 -22.962 ## - heladas 1 3.304 25.820 -21.043 ## - asesinatos 1 32.779 55.295 17.033 ## ## Step: AIC=-27.6 ## esp_vida ~ habitantes + asesinatos + estudios + heladas + densidad_pobl ## ## Df Sum of Sq RSS AIC ## - densidad_pobl 1 0.660 23.308 -28.161 ## &lt;none&gt; 22.648 -27.598 ## + ingresos 1 0.132 22.516 -25.890 ## + analfabetismo 1 0.061 22.587 -25.732 ## + area 1 0.043 22.605 -25.693 ## - habitantes 1 2.659 25.307 -24.046 ## - heladas 1 3.179 25.827 -23.030 ## - estudios 1 3.966 26.614 -21.529 ## - asesinatos 1 33.626 56.274 15.910 ## ## Step: AIC=-28.16 ## esp_vida ~ habitantes + asesinatos + estudios + heladas ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 23.308 -28.161 ## + densidad_pobl 1 0.660 22.648 -27.598 ## + ingresos 1 0.006 23.302 -26.174 ## + analfabetismo 1 0.004 23.304 -26.170 ## + area 1 0.001 23.307 -26.163 ## - habitantes 1 2.064 25.372 -25.920 ## - heladas 1 3.122 26.430 -23.877 ## - estudios 1 5.112 28.420 -20.246 ## - asesinatos 1 34.816 58.124 15.528 ## ## Call: ## lm(formula = esp_vida ~ habitantes + asesinatos + estudios + ## heladas, data = datos) ## ## Coefficients: ## (Intercept) habitantes asesinatos estudios heladas ## 7.103e+01 5.014e-05 -3.001e-01 4.658e-02 -5.943e-03 El mejor modelo resultante del proceso de selección ha sido: modelo &lt;- lm(formula = esp_vida ~ habitantes + asesinatos + estudios + heladas, data = datos) summary(modelo) ## ## Call: ## lm(formula = esp_vida ~ habitantes + asesinatos + estudios + ## heladas, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.47095 -0.53464 -0.03701 0.57621 1.50683 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.103e+01 9.529e-01 74.542 &lt; 2e-16 *** ## habitantes 5.014e-05 2.512e-05 1.996 0.05201 . ## asesinatos -3.001e-01 3.661e-02 -8.199 1.77e-10 *** ## estudios 4.658e-02 1.483e-02 3.142 0.00297 ** ## heladas -5.943e-03 2.421e-03 -2.455 0.01802 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7197 on 45 degrees of freedom ## Multiple R-squared: 0.736,\tAdjusted R-squared: 0.7126 ## F-statistic: 31.37 on 4 and 45 DF, p-value: 1.696e-12 Es recomendable mostrar el intervalo de confianza para cada uno de los coeficientes parciales de regresión: confint(lm(formula = esp_vida ~ habitantes + asesinatos + estudios + heladas, data = datos)) ## 2.5 % 97.5 % ## (Intercept) 6.910798e+01 72.9462729104 ## habitantes -4.543308e-07 0.0001007343 ## asesinatos -3.738840e-01 -0.2264135705 ## estudios 1.671901e-02 0.0764454870 ## heladas -1.081918e-02 -0.0010673977 Cada una de las pendientes de un modelo de regresión lineal múltiple (coeficientes parciales de regresión de los predictores) se define del siguiente modo: Si el resto de variables se mantienen constantes, por cada unidad que aumenta el predictor en cuestión, la variable (Y) varía en promedio tantas unidades como indica la pendiente. Para este ejemplo, por cada unidad que aumenta el predictor estudios, la esperanza de vida aumenta en promedio 0.04658 unidades, manteniéndose constantes el resto de predictores. 4.7.1.4 Validación de condiciones para la regresión múltiple lineal 4.7.1.4.1 Relación lineal entre los predictores numéricos y la variable respuesta Esta condición se puede validar bien mediante diagramas de dispersión entre la variable dependiente y cada uno de los predictores (como se ha hecho en el análisis preliminar) o con diagramas de dispersión entre cada uno de los predictores y los residuos del modelo. Si la relación es lineal, los residuos deben de distribuirse aleatoriamente en torno a 0 con una variabilidad constante a lo largo del eje X. Esta última opción suele ser más indicada ya que permite identificar posibles datos atípicos. library(ggplot2) library(gridExtra) plot1 &lt;- ggplot(data = datos, aes(habitantes, modelo$residuals)) + geom_point() + geom_smooth(color = &quot;firebrick&quot;) + geom_hline(yintercept = 0) + theme_bw() plot2 &lt;- ggplot(data = datos, aes(asesinatos, modelo$residuals)) + geom_point() + geom_smooth(color = &quot;firebrick&quot;) + geom_hline(yintercept = 0) + theme_bw() plot3 &lt;- ggplot(data = datos, aes(estudios, modelo$residuals)) + geom_point() + geom_smooth(color = &quot;firebrick&quot;) + geom_hline(yintercept = 0) + theme_bw() plot4 &lt;- ggplot(data = datos, aes(heladas, modelo$residuals)) + geom_point() + geom_smooth(color = &quot;firebrick&quot;) + geom_hline(yintercept = 0) + theme_bw() grid.arrange(plot1, plot2, plot3, plot4) Analizando las gráficas no se observa evidencia suficiente para sospechar que no se cumple la linealidad para todos los predictores. 4.7.1.4.2 Distribución normal de los residuos: qqnorm(modelo$residuals) qqline(modelo$residuals) shapiro.test(modelo$residuals) ## ## Shapiro-Wilk normality test ## ## data: modelo$residuals ## W = 0.97935, p-value = 0.525 Tanto el análisis gráfico como es test de hipótesis confirman la normalidad. 4.7.1.4.3 Variabilidad constante de los residuos (homocedasticidad): Al representar los residuos frente a los valores ajustados por el modelo, los primeros se tienen que distribuir de forma aleatoria en torno a cero, manteniendo aproximadamente la misma variabilidad a lo largo del eje X. Si se observa algún patrón específico, por ejemplo forma cónica o mayor dispersión en los extremos, significa que la variabilidad es dependiente del valor ajustado y por lo tanto no hay homocedasticidad. ggplot(data = datos, aes(modelo$fitted.values, modelo$residuals)) + geom_point() + geom_smooth(color = &quot;firebrick&quot;, se = FALSE) + geom_hline(yintercept = 0) + theme_bw() library(lmtest) bptest(modelo) ## ## studentized Breusch-Pagan test ## ## data: modelo ## BP = 6.2721, df = 4, p-value = 0.1797 De nuevo, tanto el gráfico como la prueba concluyen que no hay evidencias de falta de homocedasticidad. 4.7.1.4.4 Multicolinealidad: Matriz de correlación entre predictores: corrplot(cor(dplyr::select(datos, habitantes, asesinatos,estudios,heladas)), method = &quot;number&quot;, tl.col = &quot;black&quot;) Análisis de Inflación de Varianza (VIF): vif(modelo) ## habitantes asesinatos estudios heladas ## 1.189835 1.727844 1.356791 1.498077 No hay predictores que muestren una correlación lineal muy alta ni inflación de varianza. Autocorrelación: dwt(modelo, alternative = &quot;two.sided&quot;) ## lag Autocorrelation D-W Statistic p-value ## 1 0.02867262 1.913997 0.748 ## Alternative hypothesis: rho != 0 No hay evidencia de autocorrelación 4.7.1.4.5 Identificación de posibles valores atípicos o influyentes datos$studentized_residual &lt;- rstudent(modelo) ggplot(data = datos, aes(x = predict(modelo), y = abs(studentized_residual))) + geom_hline(yintercept = 3, color = &quot;grey&quot;, linetype = &quot;dashed&quot;) + # se identifican en rojo observaciones con residuos estandarizados absolutos &gt; 3 geom_point(aes(color = ifelse(abs(studentized_residual) &gt; 3, &#39;red&#39;, &#39;black&#39;))) + scale_color_identity() + labs(title = &quot;Distribución de los residuos studentized&quot;, x = &quot;predicción modelo&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5)) which(abs(datos$studentized_residual) &gt; 3) ## integer(0) No se identifica ninguna observación atípica. summary(influence.measures(modelo)) ## Potentially influential observations of ## lm(formula = esp_vida ~ habitantes + asesinatos + estudios + heladas, data = datos) : ## ## dfb.1_ dfb.hbtn dfb.assn dfb.estd dfb.hlds dffit cov.r cook.d hat ## Alaska 0.41 0.18 -0.40 -0.35 -0.16 -0.50 1.36_* 0.05 0.25 ## California 0.04 -0.09 0.00 -0.04 0.03 -0.12 1.81_* 0.00 0.38_* ## Hawaii -0.03 -0.57 -0.28 0.66 -1.24_* 1.43_* 0.74 0.36 0.24 ## Nevada 0.40 0.14 -0.42 -0.29 -0.28 -0.52 1.46_* 0.05 0.29 ## New York 0.01 -0.06 0.00 0.00 -0.01 -0.07 1.44_* 0.00 0.23 En la tabla generada se recogen las observaciones que son significativamente influyentes en al menos uno de los predictores (una columna para cada predictor). Las tres últimas columnas son 3 medidas distintas para cuantificar la influencia. A modo de guía se pueden considerar excesivamente influyentes aquellas observaciones para las que: Leverages (hat): Se consideran observaciones influyentes aquellas cuyos valores hat superen \\(2.5(\\frac{p+1}{n})\\), siendo \\(p\\) el número de predictores y \\(n\\) el número de observaciones. Distancia Cook (cook.d): Se consideran influyentes valores superiores a 1. La visualización gráfica de las influencias se obtiene del siguiente modo: influencePlot(modelo) ## StudRes Hat CookD ## California -0.1500614 0.38475924 0.002879053 ## Hawaii 2.5430162 0.23979244 0.363778638 ## Maine -2.2012995 0.06424817 0.061301962 ## Nevada -0.8120831 0.28860921 0.053917754 ## Washington -1.4895722 0.17168830 0.089555784 Los análisis muestran varias observaciones influyentes (estados California y Hawaii) que exceden los límites de preocupación para los valores de Leverages o Distancia Cook. Estudios más exhaustivos consistirían en rehacer el modelo sin las observaciones y ver el impacto. 4.7.1.5 Conclusión El modelo lineal múltiple \\[Esperanza de vida=5.014e−05habitantes−3.001e−01asesinatos+4.658e−02universitarios−5.943e−03heladas\\] es capaz de explicar el \\(73.6%\\) de la variabilidad observada en la esperanza de vida (\\(R2: 0.736, R2-Adjusted: 0.7126\\)). El test F muestra que es significativo (p-value: \\(1.696e-12\\)). Se satisfacen todas las condiciones para este tipo de regresión múltiple. Dos observaciones (posición California y Hawaii) podrían estar influyendo de forma notable en el modelo. 4.7.2 Ejercicio interactivo: regresión lineal multiple aplicada a marketing Use el siguiente código para iniciar el ejercicio interactivo learnr::run_tutorial(&quot;regresion_lineal_multiple&quot;, package = &quot;ModelosLinealesFC&quot;) "],
["anova.html", "5 ANOVA 5.1 Introducción 5.2 Modelo de una vía", " 5 ANOVA 5.1 Introducción El análisis de la varianza fue desarrollado por Fisher en 1925 con el objetivo de comparar entre sí varios grupos o tratamientos. Normalmente es de interés estudiar cómo se diferencian estos grupos (o tratamientos), teniendo en cuenta la incidencia de otros factores cualitativos o cuantitativos (factores ambientales), cuya influencia es eliminada mediante una adecuada descomposición de la variabilidad total de un experimento en componentes independientes que puedan atribuirse a distintas causas. Por ejemplo, una compañía agricola que emplea diversos fertilizantes desea comprobar si éstos tienen efectos diferentes sobre el rendimiento de la planta de jitomate en donde se usan estos fertilizantes. La compañía tiene a su disposición 5 tipos de fertilizantes y para comparar su eficacia fumiga, con cada uno de los fertilizantes, un cierto número de parcelas de terreno de la misma calidad y de igual superficie. Cabe mencionar que el número de parcelas a las que les corresponde cada fertilizante no ncesariamnete deben ser las mismas, es decir el fertilizante 1 se aplica a 8 parcelas mientras que el fertilizante 2 se aplicó sólo en 6. Al recoger la cosecha se mide el rendimiento de la semilla en peso de jitomates por unidad de superficie. Este peso de jitomates nos permitirá saber si los fertilizantes están teniendo efecto sobre el rendimiento. En este ejemplo se esta tratando de controlar los efectos de los otros factores al tener parcelas de igual superficie, en condiciones lo más homogéneas posible y con la planta derivada de la misma semilla, de esta maner se espera que de observarse diferencias en las cosechas se puedan atrubuir solamente al fertilizante. Esencialmente el análisis de varianza determina si la discrepancia entre las medias de los tratamientos es mayor de lo que podría esperarse razonablemente de la discrepancia existente dentro de los tratamientos. En este tipo de modelos, la variable respuesta se considera del tipo continuo, mientras que las variables experimentales o factores son variables categóricas o categorizadas en niveles. Similar al proceso de inferencia, para llegar a conclusiones estadísticas correctas es necesario observar el resultado tras la repetición del experimento en varias unidades experimentales para cada una de las diversas condiciones que indica el diseño pero lo más homogéneas posibles dentro de cada una. Esto reducirá la variabilidad y, por tanto, aumentará la capacidad estadística de detectar cambios o identificar variables influyentes. Ya que con una variabilidad muy grande respecto al error experimental no se pueden detectar diferencias entre tratamientos. En general, en todo análisis de varianza es necesario considerar tres etapas: Diseño del experimento a fin de obtener observaciones de una variable Y, combinando adecuadamente los factores incidentes. Planteamiento de hipótesis, cálculo de sumas de cuadrados (residuales, de desviación de la hipótesis, etc.) y obtención de los cocientes F. Esta parte del análisis se formula mediante la teoría de los modelos lineales. Toma de decisiones e interpretación de los resultados. Planteamiento “a posteriori” de nuevas hipótesis. 5.2 Modelo de una vía Supongamos que una variable \\(Y\\) ha sido observada bajo \\(k\\) condiciones experimentales distintas. Puede ser que las observaciones provengan de \\(k\\) poblaciones, o bien tratarse de réplicas para cada uno de los \\(k\\) niveles de un factor. Indiquemos por \\(y_{ij}\\) la réplica \\(j (j = 1, . . . , n_i)\\) en la población o nivel \\(i (i = 1, . . . , k)\\), donde \\(n_i\\) es el número de réplicas en la población o nivel \\(i\\). Entonce el conjunto de datos se puede ver como: \\[\\begin{align*} Nivel 1 \\quad y_{11}, y_{12}, . . . , y_{1n_1}\\\\ Nivel 2 \\quad y_{21}, y_{22}, . . . , y_{2n_2}\\\\ .\\\\ .\\\\ .\\\\ Nivel k \\quad y_{k1}, y_{k2}, . . . , y_{kn_k} \\end{align*}\\] Con estos datos se pueden calcular las siguientes medias: Media en la población o nivel \\(i\\): \\[y_{i.} =\\frac{1}{n_i}\\sum_{j=1}^{n_i}y_{ij}\\] Media general: \\[\\bar{y} = y_{\\cdot\\cdot} =\\frac{1}{n}\\sum_{i=1}^{k}\\sum_{j=1}^{n_i}y_{ij}\\] donde \\(n=\\sum_{i=1}^{k}n_i\\) es el número total de observaciones. El objetivo es estimar los efectos de los distintos \\(k\\) niveles y contrastar la hipótesis de que todos los niveles del factor producen el mismo efecto, frente a la alternativa de que al menos dos difieren significativamente entre sí. Es importante mencionar que con este diseño de los datos es posible proponer distintos modelos lineales que en teoría o en la formulación matemática se adaptan al diseño de los datos antes mencionado. Por ejemplo se puede proponer el siguiente modelo lineal. \\[y_{ij}=\\mu_i+\\epsilon_{ij},\\ i=1,...,k;\\ j=1,...,n_i\\] siendo \\((\\mu_1,\\mu_2,...\\mu_k)&#39;\\) el vector de parámetros y \\(\\epsilon_{ij}\\) siendo las variables aleatorias que agrupan información proveniente de la combinación del factor y la observación (o lo no considerado en \\(\\mu_i\\)). En este modelo podemos considerar la matriz \\(X\\) definida por una matriz identidad de \\(k\\)x\\(k\\) y entonce nos lleva a un modelo de la forma \\[Y=\\mu X+\\epsilon\\] Esta es la representación matricial del modelo de regresión lineal simple ya conocido. Y por lo tanto sabemos que un estimador del vector de parámetros \\(\\mu\\) es \\(\\hat\\mu=y_{i.}\\) Asimismo podriamos expresar \\(\\mu_{i}\\) como la suma de dos términos \\(\\mu\\), común a todas las observaciones, y \\(\\tau_{i}\\) específica a cada nivel, entonces el modelo propuesto sería: \\[y_{ij}=\\mu+\\tau_i+\\epsilon_{ij},\\ i=1,...,k;\\ j=1,...,n_i\\] donde \\(y_{ij}\\) es la variable aleatoria que representa la observación j-ésima del i-ésimo tratamiento (nivel i-ésimo del factor). \\(\\mu\\) es un efecto constante, común a todos los niveles, denominado media global. \\(\\tau_i\\) es la parte de \\(y_{ij}\\) debida a la acción del nivel i-ésimo, que será común a todos los elementos sometidos a ese nivel del factor. Los efectos de los factores \\(\\tau_{i}\\) son las desviaciones de la media de cada nivel con respecto a la media general, por esta razón se debe verificar la relación \\(\\sum_{i=1}^k\\tau_{i}=0\\). \\(\\epsilon_{ij}\\) son variables aleatorias que engloban un conjunto de factores, cada uno de los cuales influye en la respuesta sólo en pequeña magnitud pero que de forma conjunta debe tenerse en cuenta en la especificación y tratamiento del modelo; es decir, las perturbaciones o error experimental pueden interpretarse como las variaciones causadas por todos los factores no analizados y que dentro del mismo tratamiento variarán de unos elementos a otros. Para este modelo lineal supondremos que los errores \\(\\epsilon_{ij}\\): Tienen media cero Tienen varianza constante (hipótesis de homocedasticidad) Son estadísticamente independientes entre sí Su distribución es normal Analogamente al modelo anterior, se trata de contrastar si todos los niveles del factor producen el mismo efecto versus al menos un nivel del factor produce un efecto distinto. En ambos casos si la hipótesis de que todos los nivels producen el mismo efecto es cierta, entonces todos los niveles tienen la misma media, \\(\\mu\\), y la pertenencia a un grupo u otro es irrelevante, y podemos considerar todas las observaciones como provenientes de una única población. 5.2.1 Estimación de parámetros La hipótesis de normalidad sobre los términos de error conlleva el hecho de que las variables \\(y_{ij}\\) sean normales e independientes, por lo que es inmediato construir la función de verosimilitud asociada a la muestra \\(y=(y_{11},...,y_{1n_1},y_{21},...,y_{2n_2},...,y_{k1},...,y_{kn_k})\\) \\[L(\\mu,\\tau_i,\\sigma^2)=(2\\pi\\sigma^2)^{n/2}exp \\left( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{k}\\sum_{j=1}^{n_i}[y_{ij}-\\mu-\\tau_i]^2 \\right)\\] Los estimadores máximo-verosímiles para los parámetros \\(\\mu,\\tau_{i},\\sigma^2\\) son los valores para los cuales la función de verosimilitud alcanza su máximo. Por conveniencia, en vez de maximizar la función de verosimilitud, se maximiza el logaritmo de esta. En este caso, \\[ln(L(\\mu,\\tau_i,\\sigma^2))=-\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2)-\\frac{1} {2\\sigma^2}\\sum_{i=1}^{k}\\sum_{j=1}^{n_i}[y_{ij}-\\mu-\\tau_i]^2\\] Las derivadas parciales respecto de los parámetros del modelo son: \\[\\frac{\\partial lnL}{\\partial \\mu}= \\frac{1} {2}\\sum_{i=1}^{k}\\sum_{j=1}^{n_i}[y_{ij}-\\mu-\\tau_i] \\] \\[\\frac{\\partial lnL}{\\partial \\tau_i}= \\frac{1} {\\sigma^2}\\sum_{j=1}^{n_i}[y_{ij}-\\mu-\\tau_i] \\quad i=1,...,k \\] \\[\\frac{\\partial lnL}{\\partial \\sigma^2}= \\frac{N}{2\\sigma^2}+\\frac{1} {2(\\sigma^2)^2}\\sum_{i=1}^{k}\\sum_{j=1}^{n_i}[y_{ij}-\\mu-\\tau_i]^2\\] Igualando a cero y despejando los parámteros obtenemos los siguientes estimadores máximo verosimiles: \\[\\hat\\mu=\\bar y_{..}\\] \\[\\hat\\tau_i=\\bar y_{i.}-\\bar y{..}\\] \\[\\hat\\sigma^2=\\frac{1}{n}\\sum_{i=1}^{k}\\sum_{j=1}^{n_i}[y_{ij}-\\hat\\mu-\\hat\\tau_i]^2=\\frac{1}{n}\\sum_{i=1}^{k}\\sum_{j=1}^{n_i}[y_{ij}-\\bar y_{i.}]^2=\\frac{1}{n}\\sum_{i=1}^{k}n_is_i^2\\] donde \\(s_i^2\\) es la varianza muestral del i-ésimo nivel. 5.2.2 Residuos Los residuos se definen como las diferencias entre los valores observados \\(y_{ij}\\) y los valores previstos por el modelo \\(\\hat y_{ij}\\) y los denotamos por \\[e_{ij}=y_{ij}-\\hat y_{ij}=y_{ij}-\\hat\\mu-\\hat \\tau_{i}=y_{ij}-\\bar y_{i.}\\] Los residuos son los estimadores de los errores aleatorios \\(\\epsilon_{ij}=y_{ij}-\\mu-\\tau_i\\), los cuales son variables aleatorias no observables. Y es posible verificar que la suma de los residuos es cero. Por otro lado con esta definición de los residuos, tenemos que el estimador máximo-verosimil \\(\\hat \\sigma^2\\) se puede escribir en términos de la suma de los residuos. \\[\\hat \\sigma^2=\\frac{\\sum_{ij}e_{ij}^2}{n}\\] 5.2.3 Propiedades de los estimadores máximo verosímiles Recordaremos algunas de las propiedades de los estimadores máximo verosímiles de \\(\\mu, \\tau_{i}, \\sigma^2\\), estas propiedades son de utilidad para la descomposición de la varianza y las pruebas de hipótesis que se verán mas adelante. 5.2.3.1 Propiedades de \\(\\hat \\mu\\) \\(\\hat\\mu\\) es un estimador insesgado de \\(\\mu\\) La varianza de \\(\\hat\\mu\\) es \\(\\frac {\\sigma^2}{n}\\) \\(\\hat\\mu\\) tiene distribución normal, puesto que dicho estimador es combinación lineal de las variables \\(y_{ij}\\) y éstas son variables aleatorias independientes con distribución normal. 5.2.3.2 Propiedades de \\(\\hat \\tau_{i}\\) \\(\\hat\\tau_{i}\\) es un estimador insesgado de \\(\\tau_{i}\\) La varianza de \\(\\hat\\tau_{i}\\) es \\((n-n_i)\\frac {\\sigma^2}{nn_i}\\) \\(\\hat\\tau_{i}\\) tiene distribución normal, puesto que dicho estimador es combinación lineal de las variables \\(y_{ij}\\) y éstas son variables aleatorias independientes con distribución normal. 5.2.3.3 Propiedades de \\(\\hat \\sigma^2\\) \\(\\hat \\sigma^2\\) NO es un estimador insesgado de \\(\\sigma^2\\) \\(\\frac {n\\hat \\sigma^2}{\\sigma^2}\\sim\\chi^2_{(n-k)}\\) 5.2.4 Descomposición de la variabilidad Para comparar los efectos de los distintos niveles de un factor se emplea la técnica estadística denominada análisis de la varianza, abreviadamente ANOVA, que está basada en la descomposición de la variabilidad total de los datos en distintas componentes. Se considera la siguiente identidad: \\[y_{ij}-\\overline{y}=(y_{i.}-\\overline{y})+(y_{ij}-y_{i.})\\] Elevando al cuadrado y sumando se obtiene: \\[\\sum_{ij}(y_{ij}-\\overline{y})^2=\\sum_{ij}(y_{i.}-\\overline{y})^2+\\sum_{ij}(y_{ij}-y_{i.})^2+2\\sum_{ij}(y_{i.}-\\overline{y})(y_{ij}-y_{i.})\\] Considerando que \\[\\sum_{ij}(y_{i.}-\\overline{y})(y_{ij}-y_{i.})=\\sum_{ij}(y_{ij}-y_{i.})y_{i.}-\\sum_{ij}(y_{ij}-y_{i.})\\overline{y}=0\\] La igualdad se da por que \\(\\hat y_{ij}=\\hat\\mu_i=y_{i.}\\) entonces \\(\\sum_{ij}(y_{ij}-y_{i.})\\) representa la suma de los residuales la cual es cero. Por lo tanto la ecuación queda como: \\[\\sum_{ij}(y_{ij}-\\overline{y})^2=\\sum_{ij}(y_{i.}-\\overline{y})^2+\\sum_{ij}(y_{ij}-y_{i.})^2\\] Generalmente se hace un cambio de notación: \\(SC_{T}\\) es la suma de cuadrados total, mide la variabilidad de las observaciones del total respecto a la media global y es denotado por \\(SC_{T}=\\sum_{ij}(y_{ij}-\\overline{y})^2\\) \\(SC_{E}\\) es la suma de cuadrados entre grupos, es denotado por \\(SC_{E}=\\sum_{ij}(y_{i.}-\\overline{y})^2=\\sum_{j}n_{i}(y_{i.}-\\overline{y})^2\\) \\(SC_{D}\\) es la suma de cuadrados dentro de grupos o intragrupos, es denotado por \\(SC_{D}=\\sum_{ij}(y_{ij}-y_{i.})^2\\) De esta forma se tiene la siguiente igualdad: \\[SC_{T}=SC_{E}+SC_{D}\\] Además se observa que: \\(SC_{T}\\) tiene \\(n-1\\) grados de libertad, ya que la suma \\(\\sum_{ij}(y_{ij}-\\overline{y})^2\\) es la parte central del estimador de la varianza \\((n-1)S^2=\\frac{\\sum_{ij}(y_{ij}-\\overline{y})^2}{n-1}\\) en el cual al estimar media global con \\(\\overline{y}\\) pierde un grado de libertad. \\(SC_{E}\\) tiene \\(k-1\\) grado de libertad,ya que \\(SC_{E}=\\sum_{j}n_{i}(y_{i.}-\\overline{y})^2\\) la suma es sobre los \\(k\\) factores y se esta estimando la media global con \\(\\overline{y}\\). \\(SC_{D}\\) tiene \\(n-k\\) grados de libertad, ya que la regresión \\(SC_{D}=SC_{T}-SC_{E},\\) en el cual se ha visto que \\(SC_{T}\\) y \\(SC_{E}\\) tienen \\(n-1\\) y \\(k-1\\) grado de libertad, respectivamente, así los grados de libertad de \\(SC_{D}=n-1-(k-1),\\) entonces \\(SC_{D}\\) tienen n-k grados de libertad. Sabemos que el modelo propuesto considera que los errores tienen distribución normal, por lo tanto dado que \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\): \\(\\frac{SC_{D}}{\\sigma^2}\\sim \\chi^2_{(n-k)}.\\) Bajo el supuesto de que las medias de las \\(k\\) poblaciones son iguales entonces \\(SC_{E}/(k-1)\\) es otro estimador insesgado de \\(\\sigma^2\\) y \\(\\frac{SC_{E}}{\\sigma^2}\\sim\\chi^2_{(k-1)}\\) Debido a un resultado de probabilidad se sabe que si \\(x \\sim \\chi^2_{(n)}\\) y \\(y \\sim \\chi^2_{(m)}\\) y si \\(x\\) es independiente a \\(y\\) entonces: \\[\\frac{x/n}{y/m}\\sim F_{(n,m)}\\] El contraste estadístico de interés en este modelo, como mencionamos al principio de esta sección, es el que tiene como hipótesis nula la igualdad de medias de los tratamientos, la cual se puede expresar de la siguiente forma: \\[H_0: \\mu_1=\\mu_2=...=\\mu_k=\\mu\\] o \\[H_0: \\tau_1=\\tau_2=...=\\tau_k=0\\] Por lo tanto se puede aplicar la prueba \\(F\\) de Fisher en el análisis de varianza para probar las hipótesis de que las medias de las \\(k\\) poblaciones son iguales, quedando el estadístico F \\[F=\\frac{\\frac{SC_{E}}{k-1}}{\\frac{SC_{D}}{n-k}} \\sim F_{(k-1,n-k)}\\] El siguiente cuadro resume la información anterior, mejor conocida como Tabla ANOVA: \\[ \\begin{array}{|c| c| c| c| c|} \\hline &amp;Grados\\ de\\ libertad &amp; Suma\\ de\\ Cuadrados &amp; Cuadrado\\ Medio &amp; Prueba\\ F \\\\ \\hline \\hline Entre\\ grupos &amp; k-1 &amp; SC_{E}=\\sum_{j}n_{i}(y_{i.}-\\overline{y})^2 &amp; \\frac{SC_{E}}{k-1} &amp;\\frac{SC_{E}/(k-1)}{SC_{D}/(n-k)} \\\\ \\hline Dentro\\ grupos &amp; n-k &amp; SC_{D}=\\sum_{ij}(y_{ij}-y_{i.})^2&amp; \\frac{SC_{D}}{n-k} &amp; -\\\\ \\hline Total &amp; n-1 &amp; SC_{T}=\\sum_{ij}(y_{ij}-\\overline{y})^2 &amp; - &amp; - \\\\ \\hline \\end{array} \\] En esta tabla tenemos entonces la descomposición de la varianza observada en los datos. En la columna de suma de cuadrados tenemos la suma de cuadrados totales que y la descomposición de esta en la diferencia observada entre los grupos y la diferencia observada dentro de los grupos. En la tabla ANOVA también se incluye el estadístico de prueba F que se usa para concluir estadisticamente si los distintos niveles o grupos en los que están clasificadas las observaciones son significativamente diferentes o no. Similar a lo estudiado en los modelos de regresión lineal, en este análisis de varianza también se tiene el coeficiente de determinación, el cual con frecuencia se le asocia a la proporción de la variación explicada por los grupos, ya que \\(0&lt;SC_{E}&lt;SC_{T}\\) entonces los valores del coeficiente de determinación están entre \\(0&lt;R^2&lt;1.\\) Se define el coeficiente de determinación del modelo del como: \\[R^2=\\frac{SC_{E}}{SC_{T}}=1-\\frac{SC_{D}}{SC_{T}}.\\] 5.2.5 Ejemplo Se desean comparar dos medicamentos D (diurético), B (betabloquedor) con un producto inocuo P (placebo). Se tomó una muestra de 15 individuos hipertensos cuyas condiciones iniciales eran suficientemente homogéneas y se asignaron los tratamientos al azar. El objetivo del estudio es ver cómo actuan los tres tratamientos frente a la hipertensión, concretamente si la disminuyen. Para cubrir con el objetivo, se ha elegido la variable observable “porcentaje de descenso de la presión arterial media.” Los datos obtenidos se presentan en la siguiente tabla: D B P 22 20 10 28 28 5 30 35 0 15 19 14 17 33 18 Primero calculamos las medias por medicamento \\(y_{D.}\\), \\(y_{B.}\\) y \\(y_{P.}\\) asi como la media global \\(y_{..}\\) x=c(22,18,30,15,17,20,28,35,19,33,10,5,0,14,18) y=c(rep(&quot;D&quot;,5),rep(&quot;B&quot;,5),rep(&quot;P&quot;,5)) A=data.frame(x,y) aggregate(A$x,by=list(A$y), FUN=mean) ## Group.1 x ## 1 B 27.0 ## 2 D 20.4 ## 3 P 9.4 mean(A$x) ## [1] 18.93333 \\[y_{B.}=27.00\\] \\[y_{D.}=20.40\\] \\[y_{P.}=9.40\\] \\[y_{..}=18.93\\] Ahora calculamos \\(SC_E\\) y \\(SC_T\\) SCE=5*(27-18.93)^2+5*(20.4-18.93)^2+5*(9.4-18.93)^2 SCE ## [1] 790.5335 SCT=sum((x-mean(x))^2) SCT ## [1] 1348.933 Por lo tanto \\(SC_D\\) se puede calcular como la diferencia entre \\(SC_T\\) y \\(SC_E\\) SCD=SCT-SCE SCD ## [1] 558.3998 La estadística \\(F\\) tomará entonces el valor de Est_F=(SCE/2)/(SCD/12) Est_F ## [1] 8.494274 Y su correspondiente p-value es 1-pf(Est_F,2,12) ## [1] 0.005031862 Aplicando las funciones de R que calculan el análisis de varianza obtenemos anova(lm(x~y,data=A)) ## Analysis of Variance Table ## ## Response: x ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## y 2 790.53 395.27 8.4943 0.005032 ** ## Residuals 12 558.40 46.53 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 mod &lt;- aov(x ~ y, data = A) mod ## Call: ## aov(formula = x ~ y, data = A) ## ## Terms: ## y Residuals ## Sum of Squares 790.5333 558.4000 ## Deg. of Freedom 2 12 ## ## Residual standard error: 6.821535 ## Estimated effects may be unbalanced De los resultados es posible concluir que la diferencia entre los tres fármacos es claramente significativa. "]
]
