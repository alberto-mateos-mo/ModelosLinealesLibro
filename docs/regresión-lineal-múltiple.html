<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Regresión lineal múltiple | Modelos Lineales</title>
  <meta name="description" content="Material para el curso Modelos Lineales de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Regresión lineal múltiple | Modelos Lineales" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Material para el curso Modelos Lineales de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="github-repo" content="alberto-mateos-mo/ModelosLinealesLibro" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Regresión lineal múltiple | Modelos Lineales" />
  
  <meta name="twitter:description" content="Material para el curso Modelos Lineales de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  

<meta name="author" content="Sofía Villers Gómez" />
<meta name="author" content="David Alberto Mateos Montes de Oca" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regresión-lineal-simple.html"/>
<link rel="next" href="anova.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Modelos Lineales 2021-2</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#detalles"><i class="fa fa-check"></i><b>0.1</b> Detalles</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#complementos"><i class="fa fa-check"></i><b>0.2</b> Complementos</a>
<ul>
<li class="chapter" data-level="0.2.1" data-path="index.html"><a href="index.html#ejercicios-interactivos"><i class="fa fa-check"></i><b>0.2.1</b> Ejercicios Interactivos</a></li>
<li class="chapter" data-level="0.2.2" data-path="index.html"><a href="index.html#libro-sobre-r"><i class="fa fa-check"></i><b>0.2.2</b> Libro sobre R</a></li>
<li class="chapter" data-level="0.2.3" data-path="index.html"><a href="index.html#aplicación-de-análisis-de-datos"><i class="fa fa-check"></i><b>0.2.3</b> Aplicación de análisis de datos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#licencia"><i class="fa fa-check"></i>Licencia</a></li>
</ul></li>
<li class="part"><span><b>I Introduccion</b></span></li>
<li class="chapter" data-level="1" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>1</b> Modelos lineales</a>
<ul>
<li class="chapter" data-level="1.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#principios-de-la-modelización-estadística"><i class="fa fa-check"></i><b>1.1</b> Principios de la modelización estadística</a></li>
<li class="chapter" data-level="1.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#identificar-y-caracterizar-variables"><i class="fa fa-check"></i><b>1.2</b> Identificar y Caracterizar Variables</a></li>
<li class="chapter" data-level="1.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#tipos-de-variables-y-tipo-de-modelo"><i class="fa fa-check"></i><b>1.3</b> Tipos de variables y tipo de modelo</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#en-función-de-las-variables-explicativas"><i class="fa fa-check"></i><b>1.3.1</b> En función de las variables explicativas</a></li>
<li class="chapter" data-level="1.3.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#en-función-de-la-variable-respuesta"><i class="fa fa-check"></i><b>1.3.2</b> En función de la variable respuesta</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#el-modelo-lineal-general"><i class="fa fa-check"></i><b>1.4</b> El modelo lineal general</a></li>
</ul></li>
<li class="part"><span><b>II Modelos de Regresión</b></span></li>
<li class="chapter" data-level="2" data-path="regresión-lineal.html"><a href="regresión-lineal.html"><i class="fa fa-check"></i><b>2</b> Regresión Lineal</a>
<ul>
<li class="chapter" data-level="2.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#un-poco-de-história"><i class="fa fa-check"></i><b>2.1</b> Un poco de história</a></li>
<li class="chapter" data-level="2.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#objetivos-del-análisis-de-regresión"><i class="fa fa-check"></i><b>2.2</b> Objetivos del análisis de regresión</a></li>
<li class="chapter" data-level="2.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#el-algorítmo-de-regresión-lineal"><i class="fa fa-check"></i><b>2.3</b> El algorítmo de regresión lineal</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html"><i class="fa fa-check"></i><b>3</b> Regresión lineal simple</a>
<ul>
<li class="chapter" data-level="3.1" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#solución-al-problema-de-regresión-lineal-simple"><i class="fa fa-check"></i><b>3.1</b> Solución al problema de regresión lineal simple</a></li>
<li class="chapter" data-level="3.2" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#mínimos-cuadrados-ordinarios"><i class="fa fa-check"></i><b>3.2</b> Mínimos cuadrados ordinarios</a></li>
<li class="chapter" data-level="3.3" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#pruebas-de-hipótesis"><i class="fa fa-check"></i><b>3.3</b> Pruebas de hipótesis</a></li>
<li class="chapter" data-level="3.4" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#interpretación-de-los-parámetros"><i class="fa fa-check"></i><b>3.4</b> Interpretación de los parámetros</a></li>
<li class="chapter" data-level="3.5" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#ejercicios"><i class="fa fa-check"></i><b>3.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html"><i class="fa fa-check"></i><b>4</b> Regresión lineal múltiple</a>
<ul>
<li class="chapter" data-level="4.1" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#solución-al-problema-de-regresión-lineal-múltiple-ecuaciones-normales"><i class="fa fa-check"></i><b>4.1</b> Solución al problema de regresión lineal múltiple: Ecuaciones normales</a></li>
<li class="chapter" data-level="4.2" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#prueba-de-hipótesis"><i class="fa fa-check"></i><b>4.2</b> Prueba de hipótesis</a></li>
<li class="chapter" data-level="4.3" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#interpretación-de-parámetros"><i class="fa fa-check"></i><b>4.3</b> Interpretación de parámetros</a></li>
<li class="chapter" data-level="4.4" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#predicción-de-nuevos-valores"><i class="fa fa-check"></i><b>4.4</b> Predicción de nuevos valores</a></li>
<li class="chapter" data-level="4.5" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#coeficiente-de-determinación"><i class="fa fa-check"></i><b>4.5</b> Coeficiente de determinación</a></li>
<li class="chapter" data-level="4.6" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#evaluación-de-supuestos"><i class="fa fa-check"></i><b>4.6</b> Evaluación de supuestos</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#residuos"><i class="fa fa-check"></i><b>4.6.1</b> Residuos</a></li>
<li class="chapter" data-level="4.6.2" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#linealidad-de-los-predictores"><i class="fa fa-check"></i><b>4.6.2</b> Linealidad de los predictores</a></li>
<li class="chapter" data-level="4.6.3" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#supuestos-sobre-los-errores"><i class="fa fa-check"></i><b>4.6.3</b> Supuestos sobre los errores</a></li>
<li class="chapter" data-level="4.6.4" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#puntos-palanca"><i class="fa fa-check"></i><b>4.6.4</b> Puntos palanca</a></li>
<li class="chapter" data-level="4.6.5" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#multicolinealidad"><i class="fa fa-check"></i><b>4.6.5</b> Multicolinealidad</a></li>
<li class="chapter" data-level="4.6.6" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#relación-funcional"><i class="fa fa-check"></i><b>4.6.6</b> Relación funcional</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#aplicación-en-r"><i class="fa fa-check"></i><b>4.7</b> Aplicación en R</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#ejemplo-predicción-de-esperanza-de-vida"><i class="fa fa-check"></i><b>4.7.1</b> Ejemplo: Predicción de esperanza de vida</a></li>
<li class="chapter" data-level="4.7.2" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#ejercicio-interactivo-regresión-lineal-multiple-aplicada-a-marketing"><i class="fa fa-check"></i><b>4.7.2</b> Ejercicio interactivo: regresión lineal multiple aplicada a marketing</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Analisis de varianza</b></span></li>
<li class="chapter" data-level="5" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>5</b> ANOVA</a>
<ul>
<li class="chapter" data-level="5.1" data-path="anova.html"><a href="anova.html#introducción"><i class="fa fa-check"></i><b>5.1</b> Introducción</a></li>
<li class="chapter" data-level="5.2" data-path="anova.html"><a href="anova.html#modelo-de-una-vía"><i class="fa fa-check"></i><b>5.2</b> Modelo de una vía</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="anova.html"><a href="anova.html#estimación-de-parámetros"><i class="fa fa-check"></i><b>5.2.1</b> Estimación de parámetros</a></li>
<li class="chapter" data-level="5.2.2" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#residuos"><i class="fa fa-check"></i><b>5.2.2</b> Residuos</a></li>
<li class="chapter" data-level="5.2.3" data-path="anova.html"><a href="anova.html#propiedades-de-los-estimadores-máximo-verosímiles"><i class="fa fa-check"></i><b>5.2.3</b> Propiedades de los estimadores máximo verosímiles</a></li>
<li class="chapter" data-level="5.2.4" data-path="anova.html"><a href="anova.html#descomposición-de-la-variabilidad"><i class="fa fa-check"></i><b>5.2.4</b> Descomposición de la variabilidad</a></li>
<li class="chapter" data-level="5.2.5" data-path="anova.html"><a href="anova.html#ejemplo"><i class="fa fa-check"></i><b>5.2.5</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="anova.html"><a href="anova.html#modelo-de-dos-vías"><i class="fa fa-check"></i><b>5.3</b> Modelo de dos vías</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="anova.html"><a href="anova.html#modelo-aditivo"><i class="fa fa-check"></i><b>5.3.1</b> Modelo aditivo</a></li>
<li class="chapter" data-level="5.3.2" data-path="anova.html"><a href="anova.html#modelo-aditivo-con-interacción"><i class="fa fa-check"></i><b>5.3.2</b> Modelo aditivo con interacción</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modelos Lineales</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regresión-lineal-múltiple" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Regresión lineal múltiple</h1>
<p>La mayoría de los fenómenos reales son multicausales, por esta razón, un modelo de regresión más acorde a estudios reales es el modelo de regresión lineal múltiple, que es la generalización del modelo simple.</p>
<p>En este modelo supondremos que la variable de respuesta, <span class="math inline">\(y\)</span>, puede explicarse a través de una colección de <span class="math inline">\(k\)</span> covariables <span class="math inline">\(x_1,\dots,x_k\)</span>.</p>
<p>El modelo se escribe de la siguiente manera:</p>
<p><span class="math display">\[y_i = \beta_0+\beta_1 x_1^{(i)}+\beta_2 x_2^{(i)}+\dots++\beta_k x_k^{(i)}+\epsilon_i\]</span>
Al igual que en el caso simple, los parámetros del modelo se pueden estimar por mínimos cuadrados, con el inconveniente de que no se pueden realizar inferencias sobre ellos. Nuevamente, para poder hacer intervalos de confianza y pruebas de hipótesis sobre los verdaderos parámetros hay que suponer que el vector de errores se distribuye normal, en este caso multivariada, es decir:</p>
<p><span class="math display">\[\epsilon\sim N_n(0,\sigma^2\mathbb{I})\]</span></p>
<p>Esta estructura del error permite tener las mismas propiedades distribucionales que en regresión simple, es decir, <span class="math inline">\(y_i\)</span> se distribuye normal y <span class="math inline">\(\beta_i\)</span> tiene distribución normal, facilitando las inferencias sobre cada parámetro y la construcción de intervalos de predicción para las <span class="math inline">\(y\)</span>’s.</p>
<div id="solución-al-problema-de-regresión-lineal-múltiple-ecuaciones-normales" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Solución al problema de regresión lineal múltiple: Ecuaciones normales</h2>
<p>Las expresiones para estimar los parámetros involucrados en el modelo son:</p>
<p><span class="math display">\[\hat{\beta}=(X^TX)^{-1}X^Ty\]</span>
<span class="math display">\[\hat{\sigma}^2=\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n-p}\]</span></p>
<p>donde <span class="math inline">\(p=k+1\)</span> es el número total de parámetros en el modelo.</p>
<p>Tanto en el modelo simple como en el múltiple, la variación total de las <span class="math inline">\(y\)</span>’s se puede descomponer en una parte que explica el modelo, i.e., los <span class="math inline">\(k\)</span> regresores o variables explicativas y otra no explicada por estas variables, llamada error.</p>
<p><span class="math display">\[\sum_{i=1}^n(y_i-\bar{y})^2=\sum_{i=1}^n(\hat{y_i}-\bar{y})^2+\sum_{i=1}^n(\hat{y_i}-y_i)^2\]</span></p>
</div>
<div id="prueba-de-hipótesis" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Prueba de hipótesis</h2>
<p>La descomposición anterior ayuda para realizar la importante prueba de hipótesis:</p>
<p><span class="math display">\[H_0:\beta_1=\beta_2=\dots=\beta_k=0\ vs.\ H_a:\beta_i\neq0 \ p.a. \ i\]</span></p>
<p>misma que se realiza a través del cociente entre los errores cuadráticos medios:</p>
<p><span class="math display">\[F_0=\frac{SS_R/k}{SS_E/(n-k-1)}=\frac{MS_R}{MS_E}\sim F_{(k,n-k-1)}\]</span>
Esta estadística se desprende de la tabla de análisis de varianza, que es muy similar a la tabla ANOVA que se utiliza para hacer pruebas de hipótesis.</p>
<p>En este caso la tabla es:</p>
<table>
<thead>
<tr class="header">
<th align="center">Fuente de variación</th>
<th align="center">Grados de libertad</th>
<th align="center">Suma de cuadrados</th>
<th align="center">Cuadrados medios</th>
<th align="center">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Regresión</td>
<td align="center">k</td>
<td align="center"><span class="math inline">\(SS_R\)</span></td>
<td align="center"><span class="math inline">\(MS_R=SS_R/k\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Error</td>
<td align="center">n-k-1</td>
<td align="center"><span class="math inline">\(SS_E\)</span></td>
<td align="center"><span class="math inline">\(MS_E=SS_E/(n-k-1)\)</span></td>
<td align="center"><span class="math inline">\(F=\frac{MS_R}{MS_E}\)</span></td>
</tr>
<tr class="odd">
<td align="center">Total</td>
<td align="center">n-1</td>
<td align="center"><span class="math inline">\(S_{yy}\)</span></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Por lo general, esta estadística rechaza la hipótesis nula, ya que de lo contrario, implicaría que ninguna de las variables contribuye a explicar la respuesta, <span class="math inline">\(y\)</span>. Como se puede observar en la hipótesis alternativa, el rechazar <span class="math inline">\(H_0\)</span> solo implica que al menos uno de los regresores contribuye significativamente a explicar <span class="math inline">\(y\)</span>.</p>
<p>Asimismo, el rechazar <span class="math inline">\(H_0\)</span> no implica que todos contribuyan ni tampoco dice cuál o cuáles contribuyen, por esta razón, una salida estándar de regresión múltiple tiene pruebas individuales sobre la significancia de cada regresor en el modelo.</p>
<p>El estadístico para hacer tanto los contrastes de hipótesis como los intervalos de confianza individuales, es:</p>
<p><span class="math display">\[t=\frac{\hat{\beta_i}-\beta_0^{(i)}}{\sqrt{\hat{\mathbb{V}ar}(\hat{\beta_i})}}\sim t_{(n-p)}\]</span>
Podemos apreciar que los constrastes de hipótesis se pueden hacer contra cualquier valor particular del parámetro <span class="math inline">\(\beta_0^{(i)}\)</span>, en general. No obstante, en las pruebas estándar sobre los parámetros de un modelo, este valor particular es 0, ya que se intenta determinar si la variable asociada al <span class="math inline">\(i\)</span>-ésimo parámetro es estadísticamente significativa para explicar la respuesta.</p>
<p>Por lo que el estadístico para este caso es:</p>
<p><span class="math display">\[t=\frac{\hat{\beta_i}}{\sqrt{\hat{\mathbb{V}ar}(\hat{\beta_i})}}\sim t_{(n-p)}\]</span></p>
<p>De este estadístico se desprenden también los intervalos de confianza para cada parámetro:</p>
<p><span class="math display">\[\beta_i\in(\hat{\beta_i}\pm t_{(n-p,1-\alpha/2)} \sqrt{\hat{\mathbb{V}ar} (\hat{\beta_i})})\]</span></p>
</div>
<div id="interpretación-de-parámetros" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Interpretación de parámetros</h2>
<p>La interpretación de cada parámetro es similar a la del coeficiente de regresión <span class="math inline">\(\hat{\beta_1}\)</span> en el modelo simple, anexando la frase: “manteniendo constantes el resto de las variables.”</p>
<p>Esto es, <span class="math inline">\(\hat{\beta_i}\)</span> es el cambio promedio o cambio esperado en <span class="math inline">\(y\)</span> por unidad de cambio en <span class="math inline">\(x_i\)</span>, sin considerar cambio alguno en ninguna de las otras variables dentro del modelo, es decir, suponiendo que estas otras variables permanecen fijas. Esta interpretación es similar a la que se hace de la derivada parcial en un modelo determinista.</p>
<p>Nuevamente, la interpretación de <span class="math inline">\(\hat{\beta_0}\)</span> estará sujeta a la posibilidad de que, en este caso, todas las variables puedan tomar el valor cero.</p>
</div>
<div id="predicción-de-nuevos-valores" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Predicción de nuevos valores</h2>
<p>Uno de los usos más frecuentes del modelo de regresión es el de predecir un valor de la respuesta para un valor particular de las covariables en el modelo. Si la predicción se realiza para un valor de las covariables dentro del rango de observación de las mismas, se tratará de una interpolación, y si se realiza para un valor fuera de este rango, hablaremos de una extrapolación.</p>
<p>En cualquiera de los dos casos, estaremos interesados en dos tipos de predicciones:</p>
<ul>
<li><p>Predicción de la respuesta media: <span class="math inline">\(y_0=\mathbb{E}(y|X_0)\)</span></p></li>
<li><p>Predicción de una nueva observación: <span class="math inline">\(y_0\)</span></p></li>
</ul>
<p>En ambos casos, la estimación puntual es la misma: <span class="math inline">\(\hat{y_0}=X_0^T\hat{\beta}\)</span></p>
<p>Lo que difiere es el intervalo de predicción.</p>
<p>Para la respuesta media es: <span class="math inline">\(y_0=(\hat{y_0}\pm t_{(n-p,1-\alpha/2)}\sqrt{\hat{\sigma^2}X_0^T(X^TX)^{-1}X_0})\)</span></p>
<p>Y para predecir una observación: <span class="math inline">\(y_0=(\hat{y_0}\pm t_{(n-p,1-\alpha/2)}\sqrt{\hat{\sigma^2}(1+X_0^T(X^TX)^{-1}X_0)})\)</span></p>
</div>
<div id="coeficiente-de-determinación" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Coeficiente de determinación</h2>
<p>Un primer elemento de juicio sobre el modelo de regresión lo constituye el coeficiente de determinación <span class="math inline">\(R^2\)</span>, que es la proporción de variabilidad de las <span class="math inline">\(y\)</span>’s que es explicada por las <span class="math inline">\(x\)</span>’s y que se escribe como:</p>
<p><span class="math display">\[R^2=\frac{SS_R}{S_{yy}}=1-\frac{SS_E}{S_{yy}}\]</span>
Una <span class="math inline">\(R^2\)</span> cercana a uno implicaría que mucha de la variabilidad de la respuesta es explicada por el conjunto de regresores incluidos en el modelo.</p>
<p>Es deseable tener una <span class="math inline">\(R^2\)</span> grande en nuestro modelo, pero esto no significa, como mucha gente piensa, que ya el modelo está bien ajustado.</p>
</div>
<div id="evaluación-de-supuestos" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Evaluación de supuestos</h2>
<p>Los dos modelos de regresión presentados, el simple y el múltiple, se construyeron sobre los supuestos de:</p>
<ul>
<li><p>La relación funcional entre la variable de respuesta <span class="math inline">\(y\)</span> y cada regresor <span class="math inline">\(x_i\)</span> es lineal</p></li>
<li><p>La esperanza de los errores es cero, <span class="math inline">\(\mathbb{E}(\epsilon_i=0)\)</span></p></li>
<li><p>La varianza de los errores es constante, <span class="math inline">\(\mathbb{V}ar(\epsilon_i) = \sigma^2\)</span></p></li>
<li><p>Los errores no están correlacionados, <span class="math inline">\(\mathbb{C}ov(\epsilon_i, \epsilon_j) = 0;\ i\neq j\)</span></p></li>
<li><p>Los errores tienen distribución normal con media cero y varianza <span class="math inline">\(\sigma^2\)</span></p></li>
</ul>
<p>Entonces, para garantizar que el modelo es adecuado, es indispensable verificar estos supuestos.</p>
<div id="residuos" class="section level3" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> Residuos</h3>
<p>Los elementos más importantes para verificar estos supuestos son los residuos, definidos como:</p>
<p><span class="math display">\[e_i=y_i-\hat{y}_i\]</span></p>
<p>Estos residuos representan la discrepancia entre la respuesta predicha por el modelo ajustado, <span class="math inline">\(\hat{y}_i\)</span> y el correspondiente valor observado, <span class="math inline">\(y_i\)</span>.</p>
<p>En la literatura de regresi ́on lineal existen cuatro tipos de residuos, a saber</p>
<ul>
<li><p><strong>Residuo crudo</strong>: <span class="math inline">\(e_i\)</span></p></li>
<li><p><strong>Residuo estandarizado</strong>: <span class="math inline">\(d_i=\frac{e_i}{\sqrt{\hat{\sigma}^2}}\)</span></p></li>
<li><p><strong>Residuo estudentizado interno</strong>: <span class="math inline">\(r_i=\frac{e_i}{\sqrt{\hat{\sigma}^2(1-h_{ii}})}\)</span></p></li>
<li><p><strong>Residuo estudentizado externo</strong>: <span class="math inline">\(t_i=\frac{e_i}{\sqrt{\hat{\sigma_{(-i)}}^2(1-h_{ii})}}\)</span></p></li>
</ul>
<p>Estos residuos se utilizan en los distintos procedimientos para evaluar los supuestos y lo adecuado del ajuste del modelo. La mayoría de las pruebas conocidas para la verificación de los supuestos, son pruebas gráficas.</p>
<p>Indudablemente, la prueba más importante es sobre la normalidad de los errores, ya que sobre este supuesto descansan todas la inferencias de este modelo.</p>
<p>La manera de verificarlo es a través de la gráfica conocida como QQ-plot o QQ-norm, que grafica los cuantiles teóricos de una distribución normal (eje x) vs. los cuantiles asociados a los residuos. Entonces, si los residuos realmente provienen de una normal, la gráfica debe mostrar la función identidad. Fuertes desviaciones de esta línea darían evidencia de que los errores no se distribuyen normal.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="linealidad-de-los-predictores" class="section level3" number="4.6.2">
<h3><span class="header-section-number">4.6.2</span> Linealidad de los predictores</h3>
<p>La manera estándar de evaluar la linealidad de las variables explicativas es a través de la gráfica de cada una de ellas contra los residuos. Si la variable en cuestión ingresa al modelo de manera lineal, esta gráfica debe mostrar un patrón totalmente aleatorio entre los puntos dispuestos en ella.</p>
<p>Cuando la variable explicativa es politómica, este tipo de gráficas son poco ilustrativas en este sentido.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="supuestos-sobre-los-errores" class="section level3" number="4.6.3">
<h3><span class="header-section-number">4.6.3</span> Supuestos sobre los errores</h3>
<p>Si la gráfica entre los valores ajustados y los residuos estandarizados, muestra un patrón aleatorio, es simétrica alrededor del cero y los puntos están comprendidos entre los valores -2 y 2, entonces se tendrá evidencia de que los errores tienen media cero, varianza constante y no están correlacionados.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Los métodos mostrados hasta ahora, permiten evaluar el modelo de manera global y no por cada observación dentro del mismo. Dado que una observación puede resultar determinante sobre alguna(s) característica(s) del modelo, es conveniente verificar el impacto que cada observación pueda tener en los distintos aspectos del modelo. Las estadísticas para evaluar el impacto que tiene una observación sobre todo el vector de parámetros, alguno de los regresores y sobre los valores predichos, se basan en la misma idea, que consiste en cuantificar el cambio en la característica de interés con y sin la observación que se está evaluando.</p>
</div>
<div id="puntos-palanca" class="section level3" number="4.6.4">
<h3><span class="header-section-number">4.6.4</span> Puntos palanca</h3>
<p>Antes de presentar las estadísticas que servirán para hacer este diagnóstico, introduciremos un elemento que es común a ellas: la llamada palanca (leverage) de una observación.</p>
<p>Recordemos que el ajuste del modelo se expresaba como:</p>
<p><span class="math display">\[\hat{\beta}=(X^TX)^{-1}X^Ty \Rightarrow \hat{y}=X\hat{\beta}=Hy\]</span></p>
<p>Con <span class="math inline">\(H\)</span> conocida como la matriz sombrero.</p>
<p>Un resultado fundamental sobre esta matriz sombrero es:</p>
<p><span class="math display">\[\mathbb{V}ar(e)=(I-H)\sigma^2 \Rightarrow \mathbb{V}ar(e_i)=(1-h_i)\sigma^2\]</span>
Con <span class="math inline">\(h_i\)</span> el i-ésimo elemento de la diagonal de la matriz <span class="math inline">\(H\)</span>.</p>
<p>Observemos que esta palanca sólo depende de <span class="math inline">\(X\)</span>, entonces, una observación con una palanca, <span class="math inline">\(h_i\)</span>, grande, es aquella con valores extremos en alguna(s) de su(s) covariable(s).</p>
<p>Ya que el promedio de las <span class="math inline">\(h_i&#39;s\)</span> es <span class="math inline">\(p/n\)</span>, consideraremos una observación con palanca grande si su palanca es mayor a <span class="math inline">\(2p/n\)</span>. En este sentido, <span class="math inline">\(h_i\)</span> corresponde a la <em>distancia de Mahalanobis</em> de <span class="math inline">\(X\)</span> definida como <span class="math inline">\((X-\bar{X})^T\hat{\Sigma}^{-1}(X-\bar{X})\)</span>.</p>
<p>La dependencia de las estadísticas para el diagnóstico de las observaciones, estriba en que sus cálculos dependen de los valores de la palanca de cada individuo. Estas estadísticas son:</p>
<ul>
<li>Distancia de Cook</li>
<li>Dfbetas</li>
<li>Dffits</li>
</ul>
<p><strong>Distancia de Cook</strong>: Sirve para determinar si una observación es influyente en todo el vector de parámetros. Una observación se considera influyente, si su distancia de Cook sobrepasa el valor uno.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p><strong>Dfbetas</strong>: Sirven para determinar si una observación es influyente en alguno de los coeficientes de regresión. Hay un dfbeta por cada parámetro dentro del modelo, incluido, por supuesto, el de la ordenada al origen. La regla de dedo es que la observación <span class="math inline">\(i\)</span> es influyente en el j-ésimo coeficiente de regresión si:</p>
<p><span class="math display">\[|Dfbetas_{j,i}|&gt;\frac{2}{\sqrt{n}}\]</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-13-1.png" width="672" /><img src="_main_files/figure-html/unnamed-chunk-13-2.png" width="672" /></p>
<p><strong>Dffits</strong>: Se utilizan para determinar si una observación es influyente en la predicción de <span class="math inline">\(y\)</span>. Se dice que la i-ésima observación es influyente para predecir <span class="math inline">\(y\)</span>, si:</p>
<p><span class="math display">\[|Dffits_i|&gt;2\sqrt{\frac{p}{n}}\]</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="multicolinealidad" class="section level3" number="4.6.5">
<h3><span class="header-section-number">4.6.5</span> Multicolinealidad</h3>
<p>El modelo de regresión lineal múltiple, se construye bajo el supuesto de que los regresores son ortogonales, i.e., son independientes.</p>
<p>Desafortunadamente, en la mayoría de las aplicaciones el conjunto de regresores no es ortogonal. Algunas veces, esta falta de ortogonalidad no es seria; sin embargo, en algunas otras los regresores están muy cerca de una perfecta relación lineal, en tales casos las inferencias realizadas a través del modelo de regresión lineal pueden ser erróneas. Cuando hay una cercana dependencia lineal entre los regresores, se dice que estamos en presencia de un problema de multicolinealidad.</p>
<p><strong>Efectos de la multicolinealidad</strong>:</p>
<ul>
<li><p>Varianzas de los coeficientes estimados son muy grandes.</p></li>
<li><p>Los estimadores calculados de distintas sub muestras de la misma población, pueden ser muy diferentes.</p></li>
<li><p>La significancia de algún regresor se puede ver afectada (volverse no significativo) por que su varianza es más grande de lo que debería ser en realidad o por la correlación de la variable con el resto dentro del modelo.</p></li>
<li><p>Es común que algún signo de un parámetro cambie, haciendo ilógica su interpretación dentro del modelo.</p></li>
</ul>
<div id="cómo-detectar-multicolinealidad" class="section level4" number="4.6.5.1">
<h4><span class="header-section-number">4.6.5.1</span> ¿Cómo detectar multicolinealidad?</h4>
<p><strong>Matriz de correlación.</strong></p>
<p>Examinar las correlaciones entre pares de variables:</p>
<p><span class="math display">\[r_{ij}\ \ \  i, j = 1, 2, \dots, k\ \  i\neq j\]</span></p>
<p>Pero, si dos o más regresores están linealmente relacionados, es posible que ninguna de las correlaciones entre cada par de variables, sea grande.</p>
<p><strong>Factor de inflación de la varianza.</strong></p>
<p><span class="math display">\[VIF_j=(1-R_j^2)^{-1}\]</span>
Con <span class="math inline">\(R_j^2\)</span> el coeficiente de determinación del modelo de regresión entre el j-ésimo regresor, <span class="math inline">\(x_j\)</span> (tomado como variable de respuesta) y el resto de los regresores <span class="math inline">\(x_i\)</span>, <span class="math inline">\(i\neq j\)</span>.</p>
<p>Experiencias prácticas indican que si algunos de los VIF’s excede a 10, su coeficiente asociado es pobremente estimado por el modelo debido a multicolinealidad.</p>
<p><strong>Análisis del eigensistema.</strong></p>
<p>Basado en los eigenvalores de la matriz <span class="math inline">\(X^TX\)</span>.</p>
<p><strong>Número de condición.</strong></p>
<p><span class="math display">\[K=\frac{\lambda_{max}}{\lambda_{min}}\]</span>
Si el número de condición es menor que 100, no existen problemas serios de multicolinealidad. Si está entre 100 y 1000 existe de moderada a fuerte multicolinealidad y si excede a 1000, hay severa multicolinealidad.</p>
<p><strong>Índice de condición.</strong></p>
<p><span class="math display">\[k_j=\frac{\lambda_{max}}{\lambda_j}\]</span>
Si el índice de condición es menor que 10, no hay ningún problema. Si está entre 10 y 30, hay moderada multicolinealidad, y si es mayor que 30, existe una fuerte colinealidad en la j-ésima variable en el modelo.</p>
<p>N.B. En algunos paquetes estos índices se presentan aplicando la raíz cuadrada a su expresión, entonces hay que extraer raíz a los puntos de corte de los criterios correspondientes.</p>
</div>
</div>
<div id="relación-funcional" class="section level3" number="4.6.6">
<h3><span class="header-section-number">4.6.6</span> Relación funcional</h3>
<p>Un supuesto importante en el modelo de regresión es el que considera que debe existir una relación funcional lineal entre cada regresor y la variable de respuestas. Pero, ¿qué debemos hacer si no se cumple esta relación lineal de la respuesta con alguno(s) de los regresor(es)?</p>
<p>Primero, ya dijimos que este supuesto se evalúa realizando la gráfica de dispersión entre los residuos del modelo y los valores de la variable en cuestión. Cuando no hay una asociación lineal entre la respuesta y la covariable, generalmente este diagrama de dispersión muestra un patrón (tendencia) que sugiere qué tipo de transformación se debería hacer a la covariable para lograr linealidad con la respuesta.</p>
<p>Debe quedar claro que la transformación puede realizarse a la variable explicativa o a la variable de respuesta.</p>
<p>A muchos investigadores no les gusta transformar la respuesta porque argumentan que pierden <em>interpretabilidad</em> del modelo. Aunque esto puede ser cierto, existen transformaciones de la respuesta que pueden <em>regresarse</em> para interpretar el modelo con la respuesta original.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Un problema asociado a esta identificación por parte del usuario, es que debe tener experiencia para asociar estas formas a una función analítica específica; hecho no necesariamente cierto. Por lo tanto, requiere de alguna herramienta técnica que pudiera auxiliarlo en esta labor.</p>
<p>Un buen auxiliar, en el caso de que se crea que es necesario transformar la respuesta, es usar la llamada trasformación Box-Cox.</p>
<div id="transformación-box-cox" class="section level4" number="4.6.6.1">
<h4><span class="header-section-number">4.6.6.1</span> Transformación Box-Cox</h4>
<p>La transformación Box-Cox de la respuesta, es una función que sirve para normalizar la distribución del error, estabilizar la varianza de este error y mejorar la relación lineal entre <span class="math inline">\(y\)</span> y las <span class="math inline">\(X’s\)</span>.</p>
<p>Se define como:</p>
<p><span class="math display">\[y_i^{\lambda} = \left\{ \begin{array}{ll} \frac{y_i^{\lambda-1}}{\lambda}, &amp;  \lambda \neq 0;\\ ln(y_i), &amp; \lambda=0 .\end{array} \right.\]</span>
La siguiente tabla muestra el rango de valores de <span class="math inline">\(\lambda\)</span> que estarían asociados a una transformación analítica común.</p>
<table>
<thead>
<tr class="header">
<th align="center">Rango <span class="math inline">\(\lambda\)</span></th>
<th align="center">Transformación Asociada</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">(-2.5, -1.5]</td>
<td align="center"><span class="math inline">\(\frac{1}{y^2}\)</span></td>
</tr>
<tr class="even">
<td align="center">(-1.5, -0.75]</td>
<td align="center"><span class="math inline">\(\frac{1}{y}\)</span></td>
</tr>
<tr class="odd">
<td align="center">(-0.75, -0.25]</td>
<td align="center"><span class="math inline">\(\frac{1}{\sqrt{y}}\)</span></td>
</tr>
<tr class="even">
<td align="center">(-0.25, 0.25]</td>
<td align="center"><span class="math inline">\(ln(y)\)</span></td>
</tr>
<tr class="odd">
<td align="center">(0.25, 0.75]</td>
<td align="center"><span class="math inline">\(\sqrt{y}\)</span></td>
</tr>
<tr class="even">
<td align="center">(0.75, 1.25]</td>
<td align="center"><span class="math inline">\(y\)</span></td>
</tr>
<tr class="odd">
<td align="center">(1.25, 2.5)</td>
<td align="center"><span class="math inline">\(y^2\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="transformación-box-tidwell" class="section level4" number="4.6.6.2">
<h4><span class="header-section-number">4.6.6.2</span> Transformación Box-Tidwell</h4>
<p>Box y Tidwell implementan un proceso iterativo para encontrar la mejor transformación de las variables predictoras en el modelo de regresión lineal.</p>
<p>Definiendo como <span class="math inline">\(X_j^{\gamma_j}\)</span> la correspondiente transformación Box-Tidwell de la variable <span class="math inline">\(j\)</span>.</p>
<p>La tabla anterior para las transfomaciones analíticas de la respuesta, también aplican para estas transformaciones de los predictores.</p>
</div>
</div>
</div>
<div id="aplicación-en-r" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Aplicación en R</h2>
<div id="ejemplo-predicción-de-esperanza-de-vida" class="section level3" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> Ejemplo: Predicción de esperanza de vida</h3>
<p>Un estudio quiere generar un modelo que permita predecir la esperanza de vida media de los habitantes de una ciudad en función de diferentes variables. Se dispone de información sobre: habitantes, analfabetismo, ingresos, esperanza de vida, asesinatos, nivel de estudios, presencia de heladas, área y densidad poblacional.</p>
<p>Usaremos los datos <code>state.x77</code> disponibles dentro de R que contiene estadísticas para los 50 estados de Estados Unidos.</p>
<p>Primero modificamos los nombres de las variables para que sean más amigables y creamos una nueva variable calculada que represente la densidad poblacional del estado.</p>
<div id="analizar-la-relación-entre-variables" class="section level4" number="4.7.1.1">
<h4><span class="header-section-number">4.7.1.1</span> Analizar la relación entre variables</h4>
<p>El primer paso a la hora de establecer un modelo lineal múltiple es estudiar la relación que existe entre variables. Esta información es crítica a la hora de identificar cuáles pueden ser los mejores predictores para el modelo, qué variables presentan relaciones de tipo no lineal (por lo que no pueden ser incluidas) y para identificar colinealidad entre predictores. A modo complementario, es recomendable representar la distribución de cada variable mediante histogramas.</p>
<p>Las dos formas principales de hacerlo son mediante representaciones gráficas (gráficos de dispersión) y el cálculo del coeficiente de correlación de cada par de variables.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="regresión-lineal-múltiple.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(<span class="at">x =</span> datos), <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##               habitantes ingresos analfabetismo esp_vida asesinatos estudios heladas  area densidad_pobl
## habitantes          1.00     0.21          0.11    -0.07       0.34    -0.10   -0.33  0.02          0.25
## ingresos            0.21     1.00         -0.44     0.34      -0.23     0.62    0.23  0.36          0.33
## analfabetismo       0.11    -0.44          1.00    -0.59       0.70    -0.66   -0.67  0.08          0.01
## esp_vida           -0.07     0.34         -0.59     1.00      -0.78     0.58    0.26 -0.11          0.09
## asesinatos          0.34    -0.23          0.70    -0.78       1.00    -0.49   -0.54  0.23         -0.19
## estudios           -0.10     0.62         -0.66     0.58      -0.49     1.00    0.37  0.33         -0.09
## heladas            -0.33     0.23         -0.67     0.26      -0.54     0.37    1.00  0.06          0.00
## area                0.02     0.36          0.08    -0.11       0.23     0.33    0.06  1.00         -0.34
## densidad_pobl       0.25     0.33          0.01     0.09      -0.19    -0.09    0.00 -0.34          1.00</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="regresión-lineal-múltiple.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">multi.hist</span>(<span class="at">x =</span> datos, <span class="at">dcol =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">dlty =</span> <span class="fu">c</span>(<span class="st">&quot;dotted&quot;</span>, <span class="st">&quot;solid&quot;</span>), <span class="at">main =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Otros paquetes permiten representar a la vez los diagramas de dispersión, los valores de correlación para cada par de variables y la distribución de cada una de las variables, ese es el caso de la función <code>ggpairs</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="regresión-lineal-múltiple.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span></code></pre></div>
<pre><code>## Warning: package &#39;GGally&#39; was built under R version 4.0.4</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="regresión-lineal-múltiple.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(datos, <span class="at">lower =</span> <span class="fu">list</span>(<span class="at">continuous =</span> <span class="st">&quot;smooth&quot;</span>),</span>
<span id="cb12-2"><a href="regresión-lineal-múltiple.html#cb12-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">diag =</span> <span class="fu">list</span>(<span class="at">continuous =</span> <span class="st">&quot;barDiag&quot;</span>), <span class="at">axisLabels =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/setup-1.png" width="672" /></p>
<p>Del análisis preliminar se pueden extraer las siguientes conclusiones:</p>
<p>Las variables que tienen una mayor relación lineal con la esperanza de vida son: asesinatos (<span class="math inline">\(r=-0.78\)</span>), analfabetismo (<span class="math inline">\(r=-0.59\)</span>) y estudios (<span class="math inline">\(r=0.58\)</span>).</p>
<p>Asesinatos y analfabetismo están medianamente correlacionadas (<span class="math inline">\(r = 0.7\)</span>) por lo que posiblemente no sea útil introducir ambos predictores en el modelo.</p>
<p>Las variables habitantes, área y densidad poblacional muestran una distribución exponencial, una transformación logarítmica posiblemente haría más normal su distribución.</p>
</div>
<div id="generar-el-modelo" class="section level4" number="4.7.1.2">
<h4><span class="header-section-number">4.7.1.2</span> Generar el modelo</h4>
<p>Sabemos que hay diferentes formas (algoritmos) para llegar al modelo final más adecuado. En este caso se va a emplear el método mixto iniciando el modelo con todas las variables como predictores y realizando la selección de los mejores predictores con la medición Akaike(AIC).</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="regresión-lineal-múltiple.html#cb13-1" aria-hidden="true" tabindex="-1"></a>modelo <span class="ot">&lt;-</span> <span class="fu">lm</span>(esp_vida <span class="sc">~</span> habitantes <span class="sc">+</span> ingresos <span class="sc">+</span> analfabetismo <span class="sc">+</span> asesinatos <span class="sc">+</span></span>
<span id="cb13-2"><a href="regresión-lineal-múltiple.html#cb13-2" aria-hidden="true" tabindex="-1"></a>               estudios <span class="sc">+</span> heladas <span class="sc">+</span> area <span class="sc">+</span> densidad_pobl, <span class="at">data =</span> datos )</span>
<span id="cb13-3"><a href="regresión-lineal-múltiple.html#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(modelo)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = esp_vida ~ habitantes + ingresos + analfabetismo + 
##     asesinatos + estudios + heladas + area + densidad_pobl, data = datos)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.47514 -0.45887 -0.06352  0.59362  1.21823 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    6.995e+01  1.843e+00  37.956  &lt; 2e-16 ***
## habitantes     6.480e-05  3.001e-05   2.159   0.0367 *  
## ingresos       2.701e-04  3.087e-04   0.875   0.3867    
## analfabetismo  3.029e-01  4.024e-01   0.753   0.4559    
## asesinatos    -3.286e-01  4.941e-02  -6.652 5.12e-08 ***
## estudios       4.291e-02  2.332e-02   1.840   0.0730 .  
## heladas       -4.580e-03  3.189e-03  -1.436   0.1585    
## area          -1.558e-06  1.914e-06  -0.814   0.4205    
## densidad_pobl -1.105e-03  7.312e-04  -1.511   0.1385    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7337 on 41 degrees of freedom
## Multiple R-squared:  0.7501,	Adjusted R-squared:  0.7013 
## F-statistic: 15.38 on 8 and 41 DF,  p-value: 3.787e-10</code></pre>
<p>El modelo con todas las variables introducidas como predictores tiene una <span class="math inline">\(R^2\)</span> alta (<span class="math inline">\(0.7501\)</span>), i.e. es capaz de explicar el <span class="math inline">\(75.01%\)</span> de la variabilidad observada en la esperanza de vida.</p>
<p>El p-value del modelo es significativo (<span class="math inline">\(3.787e-10\)</span>) por lo que se puede aceptar que el modelo no es por azar, al menos uno de los coeficientes parciales de regresión es distinto de 0. Muchos de ellos no son significativos, lo que es un indicativo de que podrían no contribuir al modelo.</p>
</div>
<div id="selección-de-los-mejores-predictores" class="section level4" number="4.7.1.3">
<h4><span class="header-section-number">4.7.1.3</span> Selección de los mejores predictores</h4>
<p>En este caso se van a emplear la estrategia de stepwise mixto. El valor matemático empleado para determinar la calidad del modelo va a ser Akaike(AIC).</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="regresión-lineal-múltiple.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">step</span>(<span class="at">object =</span> modelo, <span class="at">direction =</span> <span class="st">&quot;both&quot;</span>, <span class="at">trace =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=-22.89
## esp_vida ~ habitantes + ingresos + analfabetismo + asesinatos + 
##     estudios + heladas + area + densidad_pobl
## 
##                 Df Sum of Sq    RSS     AIC
## - analfabetismo  1    0.3050 22.373 -24.208
## - area           1    0.3564 22.425 -24.093
## - ingresos       1    0.4120 22.480 -23.969
## &lt;none&gt;                       22.068 -22.894
## - heladas        1    1.1102 23.178 -22.440
## - densidad_pobl  1    1.2288 23.297 -22.185
## - estudios       1    1.8225 23.891 -20.926
## - habitantes     1    2.5095 24.578 -19.509
## - asesinatos     1   23.8173 45.886  11.707
## 
## Step:  AIC=-24.21
## esp_vida ~ habitantes + ingresos + asesinatos + estudios + heladas + 
##     area + densidad_pobl
## 
##                 Df Sum of Sq    RSS     AIC
## - area           1    0.1427 22.516 -25.890
## - ingresos       1    0.2316 22.605 -25.693
## &lt;none&gt;                       22.373 -24.208
## - densidad_pobl  1    0.9286 23.302 -24.174
## - estudios       1    1.5218 23.895 -22.918
## + analfabetismo  1    0.3050 22.068 -22.894
## - habitantes     1    2.2047 24.578 -21.509
## - heladas        1    3.1324 25.506 -19.656
## - asesinatos     1   26.7071 49.080  13.072
## 
## Step:  AIC=-25.89
## esp_vida ~ habitantes + ingresos + asesinatos + estudios + heladas + 
##     densidad_pobl
## 
##                 Df Sum of Sq    RSS     AIC
## - ingresos       1     0.132 22.648 -27.598
## - densidad_pobl  1     0.786 23.302 -26.174
## &lt;none&gt;                       22.516 -25.890
## - estudios       1     1.424 23.940 -24.824
## + area           1     0.143 22.373 -24.208
## + analfabetismo  1     0.091 22.425 -24.093
## - habitantes     1     2.332 24.848 -22.962
## - heladas        1     3.304 25.820 -21.043
## - asesinatos     1    32.779 55.295  17.033
## 
## Step:  AIC=-27.6
## esp_vida ~ habitantes + asesinatos + estudios + heladas + densidad_pobl
## 
##                 Df Sum of Sq    RSS     AIC
## - densidad_pobl  1     0.660 23.308 -28.161
## &lt;none&gt;                       22.648 -27.598
## + ingresos       1     0.132 22.516 -25.890
## + analfabetismo  1     0.061 22.587 -25.732
## + area           1     0.043 22.605 -25.693
## - habitantes     1     2.659 25.307 -24.046
## - heladas        1     3.179 25.827 -23.030
## - estudios       1     3.966 26.614 -21.529
## - asesinatos     1    33.626 56.274  15.910
## 
## Step:  AIC=-28.16
## esp_vida ~ habitantes + asesinatos + estudios + heladas
## 
##                 Df Sum of Sq    RSS     AIC
## &lt;none&gt;                       23.308 -28.161
## + densidad_pobl  1     0.660 22.648 -27.598
## + ingresos       1     0.006 23.302 -26.174
## + analfabetismo  1     0.004 23.304 -26.170
## + area           1     0.001 23.307 -26.163
## - habitantes     1     2.064 25.372 -25.920
## - heladas        1     3.122 26.430 -23.877
## - estudios       1     5.112 28.420 -20.246
## - asesinatos     1    34.816 58.124  15.528</code></pre>
<pre><code>## 
## Call:
## lm(formula = esp_vida ~ habitantes + asesinatos + estudios + 
##     heladas, data = datos)
## 
## Coefficients:
## (Intercept)   habitantes   asesinatos     estudios      heladas  
##   7.103e+01    5.014e-05   -3.001e-01    4.658e-02   -5.943e-03</code></pre>
<p>El mejor modelo resultante del proceso de selección ha sido:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="regresión-lineal-múltiple.html#cb18-1" aria-hidden="true" tabindex="-1"></a>modelo <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> esp_vida <span class="sc">~</span> habitantes <span class="sc">+</span> asesinatos <span class="sc">+</span> estudios <span class="sc">+</span></span>
<span id="cb18-2"><a href="regresión-lineal-múltiple.html#cb18-2" aria-hidden="true" tabindex="-1"></a>                heladas, <span class="at">data =</span> datos)</span>
<span id="cb18-3"><a href="regresión-lineal-múltiple.html#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(modelo)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = esp_vida ~ habitantes + asesinatos + estudios + 
##     heladas, data = datos)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.47095 -0.53464 -0.03701  0.57621  1.50683 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  7.103e+01  9.529e-01  74.542  &lt; 2e-16 ***
## habitantes   5.014e-05  2.512e-05   1.996  0.05201 .  
## asesinatos  -3.001e-01  3.661e-02  -8.199 1.77e-10 ***
## estudios     4.658e-02  1.483e-02   3.142  0.00297 ** 
## heladas     -5.943e-03  2.421e-03  -2.455  0.01802 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7197 on 45 degrees of freedom
## Multiple R-squared:  0.736,	Adjusted R-squared:  0.7126 
## F-statistic: 31.37 on 4 and 45 DF,  p-value: 1.696e-12</code></pre>
<p>Es recomendable mostrar el intervalo de confianza para cada uno de los coeficientes parciales de regresión:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="regresión-lineal-múltiple.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(<span class="fu">lm</span>(<span class="at">formula =</span> esp_vida <span class="sc">~</span> habitantes <span class="sc">+</span> asesinatos <span class="sc">+</span> estudios <span class="sc">+</span></span>
<span id="cb20-2"><a href="regresión-lineal-múltiple.html#cb20-2" aria-hidden="true" tabindex="-1"></a>             heladas, <span class="at">data =</span> datos))</span></code></pre></div>
<pre><code>##                     2.5 %        97.5 %
## (Intercept)  6.910798e+01 72.9462729104
## habitantes  -4.543308e-07  0.0001007343
## asesinatos  -3.738840e-01 -0.2264135705
## estudios     1.671901e-02  0.0764454870
## heladas     -1.081918e-02 -0.0010673977</code></pre>
<p>Cada una de las pendientes de un modelo de regresión lineal múltiple (coeficientes parciales de regresión de los predictores) se define del siguiente modo: Si el resto de variables se mantienen constantes, por cada unidad que aumenta el predictor en cuestión, la variable (Y) varía en promedio tantas unidades como indica la pendiente.</p>
<p>Para este ejemplo, por cada unidad que aumenta el predictor estudios, la esperanza de vida aumenta en promedio 0.04658 unidades, manteniéndose constantes el resto de predictores.</p>
</div>
<div id="validación-de-condiciones-para-la-regresión-múltiple-lineal" class="section level4" number="4.7.1.4">
<h4><span class="header-section-number">4.7.1.4</span> Validación de condiciones para la regresión múltiple lineal</h4>
<div id="relación-lineal-entre-los-predictores-numéricos-y-la-variable-respuesta" class="section level5" number="4.7.1.4.1">
<h5><span class="header-section-number">4.7.1.4.1</span> Relación lineal entre los predictores numéricos y la variable respuesta</h5>
<p>Esta condición se puede validar bien mediante diagramas de dispersión entre la variable dependiente y cada uno de los predictores (como se ha hecho en el análisis preliminar) o con diagramas de dispersión entre cada uno de los predictores y los residuos del modelo. Si la relación es lineal, los residuos deben de distribuirse aleatoriamente en torno a 0 con una variabilidad constante a lo largo del eje X. Esta última opción suele ser más indicada ya que permite identificar posibles datos atípicos.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="regresión-lineal-múltiple.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb22-2"><a href="regresión-lineal-múltiple.html#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb22-3"><a href="regresión-lineal-múltiple.html#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="regresión-lineal-múltiple.html#cb22-4" aria-hidden="true" tabindex="-1"></a>plot1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> datos, <span class="fu">aes</span>(habitantes, modelo<span class="sc">$</span>residuals)) <span class="sc">+</span></span>
<span id="cb22-5"><a href="regresión-lineal-múltiple.html#cb22-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">color =</span> <span class="st">&quot;firebrick&quot;</span>) <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb22-6"><a href="regresión-lineal-múltiple.html#cb22-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb22-7"><a href="regresión-lineal-múltiple.html#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="regresión-lineal-múltiple.html#cb22-8" aria-hidden="true" tabindex="-1"></a>plot2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> datos, <span class="fu">aes</span>(asesinatos, modelo<span class="sc">$</span>residuals)) <span class="sc">+</span></span>
<span id="cb22-9"><a href="regresión-lineal-múltiple.html#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">color =</span> <span class="st">&quot;firebrick&quot;</span>) <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb22-10"><a href="regresión-lineal-múltiple.html#cb22-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb22-11"><a href="regresión-lineal-múltiple.html#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="regresión-lineal-múltiple.html#cb22-12" aria-hidden="true" tabindex="-1"></a>plot3 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> datos, <span class="fu">aes</span>(estudios, modelo<span class="sc">$</span>residuals)) <span class="sc">+</span></span>
<span id="cb22-13"><a href="regresión-lineal-múltiple.html#cb22-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">color =</span> <span class="st">&quot;firebrick&quot;</span>) <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb22-14"><a href="regresión-lineal-múltiple.html#cb22-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb22-15"><a href="regresión-lineal-múltiple.html#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="regresión-lineal-múltiple.html#cb22-16" aria-hidden="true" tabindex="-1"></a>plot4 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> datos, <span class="fu">aes</span>(heladas, modelo<span class="sc">$</span>residuals)) <span class="sc">+</span></span>
<span id="cb22-17"><a href="regresión-lineal-múltiple.html#cb22-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">color =</span> <span class="st">&quot;firebrick&quot;</span>) <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb22-18"><a href="regresión-lineal-múltiple.html#cb22-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb22-19"><a href="regresión-lineal-múltiple.html#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="regresión-lineal-múltiple.html#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(plot1, plot2, plot3, plot4)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Analizando las gráficas no se observa evidencia suficiente para sospechar que no se cumple la linealidad para todos los predictores.</p>
</div>
<div id="distribución-normal-de-los-residuos" class="section level5" number="4.7.1.4.2">
<h5><span class="header-section-number">4.7.1.4.2</span> Distribución normal de los residuos:</h5>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="regresión-lineal-múltiple.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(modelo<span class="sc">$</span>residuals)</span>
<span id="cb23-2"><a href="regresión-lineal-múltiple.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(modelo<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="regresión-lineal-múltiple.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(modelo<span class="sc">$</span>residuals)</span></code></pre></div>
<pre><code>## 
## 	Shapiro-Wilk normality test
## 
## data:  modelo$residuals
## W = 0.97935, p-value = 0.525</code></pre>
<p>Tanto el análisis gráfico como es test de hipótesis confirman la normalidad.</p>
</div>
<div id="variabilidad-constante-de-los-residuos-homocedasticidad" class="section level5" number="4.7.1.4.3">
<h5><span class="header-section-number">4.7.1.4.3</span> Variabilidad constante de los residuos (homocedasticidad):</h5>
<p>Al representar los residuos frente a los valores ajustados por el modelo, los primeros se tienen que distribuir de forma aleatoria en torno a cero, manteniendo aproximadamente la misma variabilidad a lo largo del eje X.</p>
<p>Si se observa algún patrón específico, por ejemplo forma cónica o mayor dispersión en los extremos, significa que la variabilidad es dependiente del valor ajustado y por lo tanto no hay homocedasticidad.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="regresión-lineal-múltiple.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> datos, <span class="fu">aes</span>(modelo<span class="sc">$</span>fitted.values, modelo<span class="sc">$</span>residuals)) <span class="sc">+</span></span>
<span id="cb26-2"><a href="regresión-lineal-múltiple.html#cb26-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb26-3"><a href="regresión-lineal-múltiple.html#cb26-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">color =</span> <span class="st">&quot;firebrick&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb26-4"><a href="regresión-lineal-múltiple.html#cb26-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb26-5"><a href="regresión-lineal-múltiple.html#cb26-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="regresión-lineal-múltiple.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span></code></pre></div>
<pre><code>## Warning: package &#39;lmtest&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="regresión-lineal-múltiple.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bptest</span>(modelo)</span></code></pre></div>
<pre><code>## 
## 	studentized Breusch-Pagan test
## 
## data:  modelo
## BP = 6.2721, df = 4, p-value = 0.1797</code></pre>
<p>De nuevo, tanto el gráfico como la prueba concluyen que no hay evidencias de falta de homocedasticidad.</p>
</div>
<div id="multicolinealidad-1" class="section level5" number="4.7.1.4.4">
<h5><span class="header-section-number">4.7.1.4.4</span> Multicolinealidad:</h5>
<p><strong>Matriz de correlación entre predictores:</strong></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="regresión-lineal-múltiple.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">corrplot</span>(<span class="fu">cor</span>(dplyr<span class="sc">::</span><span class="fu">select</span>(datos, habitantes, asesinatos,estudios,heladas)),</span>
<span id="cb34-2"><a href="regresión-lineal-múltiple.html#cb34-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">method =</span> <span class="st">&quot;number&quot;</span>, <span class="at">tl.col =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p><strong>Análisis de Inflación de Varianza (VIF):</strong></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="regresión-lineal-múltiple.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(modelo)</span></code></pre></div>
<pre><code>## habitantes asesinatos   estudios    heladas 
##   1.189835   1.727844   1.356791   1.498077</code></pre>
<p>No hay predictores que muestren una correlación lineal muy alta ni inflación de varianza.</p>
<p><strong>Autocorrelación:</strong></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="regresión-lineal-múltiple.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dwt</span>(modelo, <span class="at">alternative =</span> <span class="st">&quot;two.sided&quot;</span>)</span></code></pre></div>
<pre><code>##  lag Autocorrelation D-W Statistic p-value
##    1      0.02867262      1.913997    0.78
##  Alternative hypothesis: rho != 0</code></pre>
<p>No hay evidencia de autocorrelación</p>
</div>
<div id="identificación-de-posibles-valores-atípicos-o-influyentes" class="section level5" number="4.7.1.4.5">
<h5><span class="header-section-number">4.7.1.4.5</span> Identificación de posibles valores atípicos o influyentes</h5>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="regresión-lineal-múltiple.html#cb39-1" aria-hidden="true" tabindex="-1"></a>datos<span class="sc">$</span>studentized_residual <span class="ot">&lt;-</span> <span class="fu">rstudent</span>(modelo)</span>
<span id="cb39-2"><a href="regresión-lineal-múltiple.html#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="regresión-lineal-múltiple.html#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> datos, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">predict</span>(modelo), <span class="at">y =</span> <span class="fu">abs</span>(studentized_residual))) <span class="sc">+</span></span>
<span id="cb39-4"><a href="regresión-lineal-múltiple.html#cb39-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">3</span>, <span class="at">color =</span> <span class="st">&quot;grey&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb39-5"><a href="regresión-lineal-múltiple.html#cb39-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># se identifican en rojo observaciones con residuos estandarizados absolutos &gt; 3</span></span>
<span id="cb39-6"><a href="regresión-lineal-múltiple.html#cb39-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color =</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(studentized_residual) <span class="sc">&gt;</span> <span class="dv">3</span>, <span class="st">&#39;red&#39;</span>, <span class="st">&#39;black&#39;</span>))) <span class="sc">+</span></span>
<span id="cb39-7"><a href="regresión-lineal-múltiple.html#cb39-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_identity</span>() <span class="sc">+</span></span>
<span id="cb39-8"><a href="regresión-lineal-múltiple.html#cb39-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Distribución de los residuos studentized&quot;</span>,</span>
<span id="cb39-9"><a href="regresión-lineal-múltiple.html#cb39-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;predicción modelo&quot;</span>) <span class="sc">+</span> </span>
<span id="cb39-10"><a href="regresión-lineal-múltiple.html#cb39-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="regresión-lineal-múltiple.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which</span>(<span class="fu">abs</span>(datos<span class="sc">$</span>studentized_residual) <span class="sc">&gt;</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## integer(0)</code></pre>
<p>No se identifica ninguna observación atípica.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="regresión-lineal-múltiple.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">influence.measures</span>(modelo))</span></code></pre></div>
<pre><code>## Potentially influential observations of
## 	 lm(formula = esp_vida ~ habitantes + asesinatos + estudios +      heladas, data = datos) :
## 
##            dfb.1_ dfb.hbtn dfb.assn dfb.estd dfb.hlds dffit   cov.r   cook.d hat    
## Alaska      0.41   0.18    -0.40    -0.35    -0.16    -0.50    1.36_*  0.05   0.25  
## California  0.04  -0.09     0.00    -0.04     0.03    -0.12    1.81_*  0.00   0.38_*
## Hawaii     -0.03  -0.57    -0.28     0.66    -1.24_*   1.43_*  0.74    0.36   0.24  
## Nevada      0.40   0.14    -0.42    -0.29    -0.28    -0.52    1.46_*  0.05   0.29  
## New York    0.01  -0.06     0.00     0.00    -0.01    -0.07    1.44_*  0.00   0.23</code></pre>
<p>En la tabla generada se recogen las observaciones que son significativamente influyentes en al menos uno de los predictores (una columna para cada predictor). Las tres últimas columnas son 3 medidas distintas para cuantificar la influencia.</p>
<p>A modo de guía se pueden considerar excesivamente influyentes aquellas observaciones para las que:</p>
<ul>
<li><p><strong>Leverages (hat):</strong> Se consideran observaciones influyentes aquellas cuyos valores hat superen <span class="math inline">\(2.5(\frac{p+1}{n})\)</span>, siendo <span class="math inline">\(p\)</span> el número de predictores y <span class="math inline">\(n\)</span> el número de observaciones.</p></li>
<li><p><strong>Distancia Cook (cook.d):</strong> Se consideran influyentes valores superiores a 1.</p></li>
</ul>
<p>La visualización gráfica de las influencias se obtiene del siguiente modo:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="regresión-lineal-múltiple.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">influencePlot</span>(modelo)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<pre><code>##               StudRes        Hat       CookD
## California -0.1500614 0.38475924 0.002879053
## Hawaii      2.5430162 0.23979244 0.363778638
## Maine      -2.2012995 0.06424817 0.061301962
## Nevada     -0.8120831 0.28860921 0.053917754
## Washington -1.4895722 0.17168830 0.089555784</code></pre>
<p>Los análisis muestran varias observaciones influyentes (estados California y Hawaii) que exceden los límites de preocupación para los valores de Leverages o Distancia Cook. Estudios más exhaustivos consistirían en rehacer el modelo sin las observaciones y ver el impacto.</p>
</div>
</div>
<div id="conclusión" class="section level4" number="4.7.1.5">
<h4><span class="header-section-number">4.7.1.5</span> Conclusión</h4>
<ul>
<li><p>El modelo lineal múltiple <span class="math display">\[Esperanza de vida=5.014e−05habitantes−3.001e−01asesinatos+4.658e−02universitarios−5.943e−03heladas\]</span> es capaz de explicar el <span class="math inline">\(73.6%\)</span> de la variabilidad observada en la esperanza de vida (<span class="math inline">\(R2: 0.736, R2-Adjusted: 0.7126\)</span>).</p></li>
<li><p>El test F muestra que es significativo (p-value: <span class="math inline">\(1.696e-12\)</span>).</p></li>
<li><p>Se satisfacen todas las condiciones para este tipo de regresión múltiple.</p></li>
<li><p>Dos observaciones (posición California y Hawaii) podrían estar influyendo de forma notable en el modelo.</p></li>
</ul>
</div>
</div>
<div id="ejercicio-interactivo-regresión-lineal-multiple-aplicada-a-marketing" class="section level3" number="4.7.2">
<h3><span class="header-section-number">4.7.2</span> Ejercicio interactivo: regresión lineal multiple aplicada a marketing</h3>
<p>Use el siguiente código para iniciar el ejercicio interactivo</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="regresión-lineal-múltiple.html#cb46-1" aria-hidden="true" tabindex="-1"></a>learnr<span class="sc">::</span><span class="fu">run_tutorial</span>(<span class="st">&quot;regresion_lineal_multiple&quot;</span>, <span class="at">package =</span> <span class="st">&quot;ModelosLinealesFC&quot;</span>)</span></code></pre></div>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="regresión-lineal-simple.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="anova.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
